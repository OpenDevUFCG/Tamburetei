# NLP em uma pÃ¡gina

# Fundamentos

## O que Ã© Aprendizagem de MÃ¡quina

> A computer program is said to learn from experience Ewith respect to some task Tand some performance measure P, if its performance on T, as measured by P, improves with experience E.â€ â€“ Tom Mitchell, 1997
> 

### AM Supervisionada

- Dados rotulados com a resposta â€œcertaâ€
- Ã‰ possÃ­vel fazer:
    - RegressÃ£o, quando a variÃ¡vel alvo Ã© numÃ©rica
    - ClassificaÃ§Ã£o, quando a variÃ¡vel alvo Ã© categÃ³rica

### AM NÃ£o Supervisionada

- Utilizada para descobrir padrÃµes interessantes nos dados, atravÃ©s de agrupamentos (clusters)

## Processamento de Linguagem Natural

Processamento de linguagem natural Ã© uma Ã¡rea de pesquisa em CiÃªncia da ComputaÃ§Ã£o e IA preocupada com o processamento de linguagens naturais como PortuguÃªs, InglÃªs e Mandarim.

## Lei de Zipf

- poucas palavras ocorrem com muita frequÃªncia enquanto muitas palavras ocorrem raramente
- O rank (r) de uma palavra vezes sua frequÃªncia (f) Ã© aproximadamente uma constante (k).

### Qual a proporÃ§Ã£o de palavras com uma dada frequÃªncia?

- Uma palavra que ocorre n vezes tem rank r(n)= k/n
- EntÃ£o, proporÃ§Ã£o com frequÃªncia n Ã© 1/n(n+1).

---

## TÃ©cnicas de PrÃ©-Processamentos

Para trabalhar com textos, Ã© necessÃ¡rio um prÃ©-processamento para colocÃ¡-lo em um formato adequado, algumas das tÃ©cnicas sÃ£o:

- TokenizaÃ§Ã£o
    
    Quebra o texto em tokens, que podem ser palavras, frases ou caracteres.
    
    PorÃ©m, muita informaÃ§Ã£o pode ser perdida: palavras pequenas, com hifens, siglas e contexto.
    

- LematizaÃ§Ã£o
    
    TÃ©cnica de processamento de texto que transforma substantivos no plural para o singular e conjugaÃ§Ãµes e variaÃ§Ãµes verbais para o infinitivo
    

- Stop Words
    
    Stopwords sÃ£o normalmente as palavras mais frequentes em uma coleÃ§Ã£o de documentos textuais e que sozinhas carregam pouco sentido.
    

- Stemming
    
    Stemmers tentam reduzir variaÃ§Ãµes morfolÃ³gicas Ã  um mesmo radical.
    
    Ex: Casa, Casinha e CasÃ£o. Resultado: Cas.
    
- N-gramas
    
    Utilizadas para reconhecer frases: frase Ã© qualquer sequÃªncia de **n** palavras.
    
    N-gramas frequentes sÃ£o mais provaveis de serem palavras com significado.
    

## Modelo Vetorial

Um vocabulÃ¡rio Ã© representado em forma de vetor e Ã© possÃ­vel analisar a assimilabilidade atravÃ©s de quÃ£o prÃ³ximo de um vetor determinado documento se encontra.

- Modelo BinÃ¡rio (Bag of Words)
    - 1: se a palavra estÃ¡ presente; 0: caso o contrario.
    - Cada documento Ã© representado por um vetor, e todos tem o mesmo o tamanho.
    - A maior limitaÃ§Ã£o do modelo binÃ¡rio Ã© que ele provÃª os mesmos pesos palavras independente das frequÃªncias de ocorrÃªncia.

- Modelo Termo Frequente(TF)
    - ğ‘¡ğ‘“(ğ‘¤i): Contagem da palavra ğ‘¤i no documento âƒ—ğ‘‘.
    - Atribuir o mesmo valor para todos os termos pode ocultar a importÃ¢ncia do termo no documento.
    - Mas, podemos dar peso muito grande stopwords e por isso, dois documentos podem ser considerados altamente similares somente porque eles usam as mesmas stopwords.

- Modelo TF-IDF
    - Inverse Document Frequency
    - Foi feito para penalizar palavras muito frequentes em diversos documentos.
    - IDF(w) = log[ (M+1)/k ]
        - M: Docs na coleÃ§Ã£o e
        - K: docs contendo w

# Aprendizagem NÃ£o Supervisionada

## Agrupamento

Um dos padrÃµes importantes que percebemos e usamos para entender/descrever dados Ã© o de *subconjuntos dos dados com alta semelhanÃ§a* entre si. No jargÃ£o que usaremos aqui, **grupos de dados** semelhantes entre si.

Algoritmos de agrupamento servem principalmente para ajudar um analista a reduzir um conjunto de dados a um conjunto menor de grupos. Isso facilita compreensÃ£o e comunicaÃ§Ã£o.

## Encontrando agrupamentos

Uma forma de descrever estrutura nos dados Ã© percebendo grupos de observaÃ§Ãµes mais semelhantes entre si que com o restante dos dados.

O tipo de pergunta que queremos responder com agrupamento Ã© *existem grupos de dados claramente diferentes em termos de var_1, var_2... e var_n?*

### Sempre hÃ¡ grupos?

NÃ£o.

## Utilizando k-means

Ele encontra k grupos, baseado na variÃ¡vel n_clusters.

<aside>
ğŸ’¡ Encontrar esses grupos, seria humanamente impossÃ­veis, entÃ£o os algoritmos de agrupamento procuram um bom ponto de divisÃ£o mas nÃ£o Ã© necessariamente o ideal.

</aside>

```python
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=4)
kmeans.fit(df_ex)
grupo = kmeans.predict(df_ex)

df_agrupado = df_ex.assign(grupo = grupo)

df_agrupado
```

O algoritmo Ã© conceitualmente bem simples; Ã© uma iteraÃ§Ã£o com duas fases: dado um conjunto de *k* centros de grupos

1. atribui cada ponto ao grupo cujo centro estÃ¡ mais
 prÃ³ximo, e 
2. move os centros para o meio dos pontos que compÃµem seu 
grupo.

## Escolhendo o k no k-means

- QuÃ£o longe meu ponto estÃ¡ do centro do cluster?
    
    Uma medida de heterogeneidade dos grupos Ã© o SSD:
    
    <aside>
    ğŸ“ SSD = âˆ‘(*xi*âˆ’*ci*)^2
    
    </aside>
    

### O mÃ©todo do cotovelo

O mÃ©todo mais comum Ã© usar uma mÃ©trica de heterogeneidade dentro dos clusters. Com uma mÃ©trica desse tipo, uma soluÃ§Ã£o com k maior sempre serÃ¡ mais homogÃªnea. O mÃ©todo entÃ£o consiste de examinar vÃ¡rios valores de *k* e *escolher um valor a partir do qual a heterogeneidade para de mudar consideravelmente*.

```python
qualidade = pd.DataFrame(columns = ['k', 'ssd'])
for k in range(1,10):
  kmeans = KMeans(n_clusters=k, n_init=20)
  kmeans.fit(df)
  qualidade = qualidade.append({'k': k, 'ssd' : kmeans.inertia_}, ignore_index=True)
```

**E quando nÃ£o hÃ¡ estrutura de grupos?**

Quanto menos estrutura, menos o joelho ficarÃ¡ aparente.

# Modelagem de TÃ³picos com LDA

Quando descrevemos textos, frequentemente o fazemos usando alÃ©m das palavras do texto tambÃ©m quais sÃ£o os assuntos ou **tÃ³picos** de que um texto trata. **LDA descobre grupos de palavras que sÃ£o utilizadas em conjunto e aproxima esses tÃ³picos.**

LDA funciona baseado em frequÃªncias de palavras, entÃ£o usaremos TFs, e nÃ£o TF-IDFs.

LDA tem como objetivo entregar as distribuiÃ§Ãµes:

- da probabilidade de tÃ³picos em um documento
- a probabilidade de palavras dentro de um tÃ³pico

## Encontrando tÃ³picos

O resultado terÃ¡

- uma matriz que descreve a relaÃ§Ã£o entre palavras e tÃ³picos
- uma matriz que descreve a relaÃ§Ã£o entre documentos e tÃ³picos

```python
lda = LatentDirichletAllocation(n_components=15, 
                                learning_method='online', # 'online' equivale a minibatch no k-means
                                random_state=0)
lda.fit(vec_text)
doc_topic_matrix = lda.transform(vec_text)
```

# Aprendizagem Supervisionada

## RegressÃ£o LogÃ­stica

Ã‰ a rede neural mais simples que pode ser encontrada na computaÃ§Ã£o, utilizando variÃ¡veis numÃ©ricas seguindo o seguinte formato:

![Untitled](img/Untitled.png)

A coluna sentimento, Ã© o equivalente ao label dessa linha. Importante para modelos supervisionados.

$$
ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ (ğ’™) = 1.0 âˆ— ğ‘¥â€™âˆ’ 1.5 âˆ— ğ‘¥â€
$$

O principio Ã© da regressÃ£o logÃ­stica Ã© achar uma funÃ§Ã£o que melhor descreva o comportamento dos dados e possa separÃ¡-los linearmente.

FunÃ§Ã£o de custo: calcula quÃ£o longe os pontos estÃ£o da reta traÃ§ada pela minha funÃ§Ã£o.