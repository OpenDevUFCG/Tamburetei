# RESUMO PROGRAMA√á√ÉO CONCORRENTE - PARTE 1

**LINKS √öTEIS**

V√≠deo aulas sobre programa√ß√£o concorrente:

[Programa√ß√£o Concorrente: Defini√ß√£o de processo e multiprograma√ß√£o](https://www.youtube.com/watch?v=lvcJxv-teC0&list=PLPrYObOisEDHo0zGqngAXoVQD5TaDtxzk)

V√≠deo aula sobre Lei de Amdahl com exemplos de quest√µes

[Lei de Amdahl](https://www.youtube.com/watch?v=QApHDvt8Hm4)

V√≠deo aulas sobre implementa√ß√µes de travas

[Sistemas Operacionais - Aula 09 - Solu√ß√µes de Espera Ocupada](https://www.youtube.com/watch?v=6wFCoa7x9BY)

MATERIAL EXTRA SOBRE SOBRECARGA

[](http://wiki.icmc.usp.br/images/e/e7/Aula-06-Avaliacao-Desempenho-Programas-Paralelos.pdf)

Modelo fork/join

[[FSPD] 05.5d: fork/join](https://www.youtube.com/watch?v=yFdTEDrme4Q)

Granularidade 

[[FSPD] 05.5a: granularidade do paralelismo](https://www.youtube.com/watch?v=T5GZqyLFjHc&list=PL-blDbur9o_4oEFxlqSIOzP2y3lK6V8Ol)

V√≠deos explicativos sobre os algoritmos

[W6 L4 Bakery Algorithm](https://www.youtube.com/watch?v=3pUScfud9Sg&ab_channel=IntroductiontoOperatingSystems)

[W6 L3 Software solutions for critical sections](https://www.youtube.com/watch?v=UuSswiH7jpE&ab_channel=IntroductiontoOperatingSystems)

Opera√ß√µes at√¥micas em Java

[Evite SYNCHRONIZED usando opera√ß√µes AT√îMICAS!](https://www.youtube.com/watch?v=hSCJFSm4QQM)

Interrup√ß√µes, travas e opera√ß√µes at√¥micas

[14.2 - Implementando Opera√ß√µes at√¥micas [SO UFAM]](https://www.youtube.com/watch?v=znpsblqoouw)

## Revis√£o de SO

### Processos e Threads

Podemos entender um **processo** como um **programa em execu√ß√£o** em uma m√°quina. Um processo **pode criar processos filhos**.

<aside>
üí° Em um computador com **uma √∫nica CPU** s√≥ √© poss√≠vel executar **um √∫nico processo por vez**.

</aside>

No caso exemplificado acima, como a CPU s√≥ tem um n√∫cleo e √© mono processada, os processos est√£o **concorrendo** pelos recursos de uma mesma CPU o tempo inteiro.  

Esse processo de concorr√™ncia ocorre em um per√≠odo de tempo t√£o pequeno que ocasiona um efeito de **pseudoparalelismo**, isto √©, o sistema n√£o est√° executando processos de maneira paralela, por√©m esses s√£o executados de uma maneira t√£o r√°pida que para o usu√°rio do sistema computacional √© praticamente simult√¢neo.

Ainda sobre processos, √© v√°lido destacar que **os processos** **s√£o independentes entre si**, isto √©, cada um tem suas **regi√µes de recursos n√£o compartilhadas**. Processos podem compartilhar essas regi√µes de recurso, como mem√≥ria, por exemplo, mas n√£o por padr√£o. 

J√° em rela√ß√£o as **threads**, essas compartilham suas regi√µes de recurso com a thread ou processo principal que a criou. Os processos filhos criados por um processo principal compreendem uma c√≥pia exata do processo pai, para alterar essa condi√ß√£o √© necess√°rio que uma instru√ß√£o do tipo exec seja realizada. Nas threads isso n√£o necessariamente ocorre, dado que um fluxo de execu√ß√£o da thread pode executar um c√≥digo independente.  

## Programa√ß√£o Concorrente - parte 1

## Paralelismo e Concorr√™ncia

**Paralelismo** √© a capacidade de conseguir lidar com v√°rias execu√ß√µes ao mesmo tempo. J√° a **concorr√™ncia** diz respeito a ter que lidar com v√°rias execu√ß√µes no mesmo momento que est√£o concorrendo pelos recursos. 

Desse modo podemos ter sistemas que s√£o **paralelos e concorrentes**, **apenas paralelos** ou **apenas concorrentes**.

Sistemas **apenas paralelos** t√™m suas rotinas completamente independentes, isto √©, cada thread ou processo em um sistema genuinamente paralelo executa de maneira que **n√£o concorrem por recursos**.

Em sistemas **apenas concorrentes**, as rotinas est√£o **o tempo todo concorrendo pelos recursos** para conseguirem realizar suas atividades. Tome como exemplo um sistema computacional monoprocessado, no qual cada processo precisa ganhar completamente a √∫nica CPU para que consiga executar, o que causa um ambiente de constante concorr√™ncia.

Por fim, sistemas que s√£o **ao mesmo tempo concorrentes e paralelos** somam caracter√≠sticas das duas descri√ß√µes acima: os processos e threads executam de maneira independente, mas est√£o concorrendo por recursos dentro do sistema computacional. Nesse cen√°rio, as rotinas podem concorrer por mem√≥ria, processamento, dentre outros recursos. 

## Por que processar de forma paralela?

As principais motiva√ß√µes para programar de forma paralela s√£o:

- Aumentar o n√≠vel de uso dos recursos dispon√≠veis (evitar deixar estruturas ociosas)
- Melhorar o desempenho da execu√ß√£o diminuindo o tempo (explorar partes paraleliz√°veis do c√≥digo)
- Algumas aplica√ß√µes s√£o inerentemente paralelas (por exemplo alguns tipos de simuladores, sistemas que atuam como dispatchers, necessidades da l√≥gica do contexto)
- Quanto melhor se aproveitam os recursos dispon√≠veis, menor √© o gasto energ√©tico

**Algumas justificativas**

- O limite da Lei de Moore
    
    Sabemos que a Lei de Moore define que **o n√∫mero de transistores dos chips dobra a cada intervalo de 18 meses, aumentando a capacidade de processamento dos mesmos**. Essa lei de manteve (e ainda resiste em alguns aspectos) por muito tempo, at√© que come√ßamos a atingir as barreiras f√≠sicas. Alguns fabricantes j√° chegaram a atingir as barreiras t√©rmicas relacionadas √† quantidade de transistores por chip. Com rela√ß√£o ao desempenho, at√© meados dos anos 2000, os processadores tinham um crescimento de cerca de 52% da performance por ano, por√©m ap√≥s essa data, o crescimento anual diminuiu significativamente para cerca de 21% por ano. 
    
- O tamanho dos circuitos
    
    Quanto mais se diminui o tamanho de um transistor para que seja poss√≠vel aumentar a quantidade em um chip, mais complicado fica o design do sistema f√≠sico, al√©m do aumento de problemas relacionados √† interfer√™ncia e a capacit√¢ncia parasita, dado a curta dist√¢ncia dos enlaces. 
    
- O tamanho dos transistores
    
    √â fato que a hist√≥ria conseguiu diminuir tanto os transistores ao ponto de conseguirmos colocar bilh√µes deles em uma pequena √°rea. Por√©m, a diminui√ß√£o do tamanho dos transistores faz com que o seu funcionamento tamb√©m seja comprometido. Para entender melhor essa justificativa, lembre-se que um transistor √© nada mais nada menos que uma chave. Essa chave tem seu funcionamento abalado pelo tamanho dos seus canais, dado que quanto menores pior √© esse funcionamento. 
    
- Densidade de pot√™ncia
    
    Quanto mais transistores colocamos em uma pequena √°rea, maior ser√° o seu aquecimento. Algumas escolhas de design, como a diminui√ß√£o da pot√™ncia dos transistores de 5v para 1v, auxiliou nessa melhoria. Al√©m disso, quanto mais se perde energia na forma de calor, maior √© o gasto energ√©tico e o desperd√≠cio e menor √© a efici√™ncia energ√©tica. 
    

Podemos concluir com essas justificativas que **n√£o conseguimos construir m√°quinas arbitrariamente r√°pidas**, dado o foco do uso da programa√ß√£o paralela como desempenho. 

- Representa√ß√µes mais fi√©is do mundo real
    
    O mundo real se apresenta de maneira paralela, e n√£o apenas isso, mas de maneira concorrente. Os indiv√≠duos atuam de maneira independente podendo interagir uns com os outros em um mesmo espa√ßo de tempo e espa√ßo. Sistemas que t√™m como finalidade essa simula√ß√£o de representa√ß√£o do mundo real t√™m foco n√£o apenas em paralelismo, mas tamb√©m em concorr√™ncia. 
    

### Efici√™ncia energ√©tica e programa√ß√£o concorrente

Alguns pontos discutidos acima mostram que utilizar de paralelismo na solu√ß√£o de um problema pode ser bastante **ben√©fico para melhorar a efici√™ncia energ√©tica do sistema**: **utilizar todos os recursos dispon√≠veis** impede que existam estruturas ociosas que ir√£o apenas consumir energia sem produzir **trabalho √∫til**, por exemplo. 

Por outro lado, alguns **problemas inerentes das arquiteturas** atuais promovem a **inefici√™ncia energ√©tica dos processadores**, como a diminui√ß√£o dos circu√≠tos e o aumento exponencial do n√∫mero de transistores, aumentando, respectivamente, o **√≠ndice de capacit√¢ncia parasita** do sistema e de **perda de energia √∫til na forma de calor**, fazendo com que haja **diminui√ß√£o da efici√™ncia**. 

Outros fatores que podem ser mal√©ficos para a efici√™ncia energ√©tica do sistema s√£o relacionados com **escolhas no momento da programa√ß√£o**. Algumas implementa√ß√µes de travas, principalmente por quando existe **exclus√£o m√∫tua por espera ocupada**, podem ocasionar n√≠veis de inefici√™ncia para a totalidade do sistema computacional. **A espera ocupada faz com que uma rotina aguarde para entrar em uma regi√£o cr√≠tica sem realiza√ß√£o de trabalho √∫til (apenas ‚Äúqueimando CPU‚Äù)**, fazendo com que o processador respons√°vel por executar aquela rotine disperdice aqueles instantes. 

### Paralelizando mem√≥rias: Mem√≥ria Distribu√≠da vs Mem√≥ria Compartilhada

A principal diferen√ßa entre a abordagem de **mem√≥ria distribu√≠da** e **mem√≥ria compartilhada** √© relacionada √†s seguintes descri√ß√µes:

‚Üí **Mem√≥ria distribu√≠da**: cada por√ß√£o de processamento tem **acesso a um recurso de mem√≥ria exclusivo**, que podem ser acessados por outros n√∫cleos, mas apenas via **mensagem expl√≠cita**.

![Captura de tela de 2022-06-28 15-07-25.png](assets_resumo_3/Captura_de_tela_de_2022-06-28_15-07-25.png)

‚Üí **Mem√≥ria compartilhada**: na mem√≥ria compartilhada, **todos os n√∫cleos t√™m acesso a uma mesma mem√≥ria**, podendo ler e escrever nos mesmos particionamentos.

 

![Captura de tela de 2022-06-28 15-08-46.png](assets_resumo_3/Captura_de_tela_de_2022-06-28_15-08-46.png)

**Mem√≥rias compartilhadas** s√£o inerentemente mais **f√°ceis de programas**, por√©m bem mais **dif√≠ceis de construir**. Isso se √© dado pelo fato de que mem√≥rias compartilhadas necessitam de **sincroniza√ß√£o**, dado que toda sua implementa√ß√£o √© realizada de maneira abstra√≠da, sem a necessidade de comunica√ß√£o expl√≠cita. Por exemplo, em um cen√°rio como esse, duas threads sendo executadas em dois n√∫cleos diferentes podem tentar escrever em um mesmo arquivo, mas, para impedir que uma inscri√ß√£o sobreponha a outra de maneira prejudicial, podemos implementar mecanismos de sincroniza√ß√£o, extremamente necess√°rios para esse caso.

J√° as **mem√≥rias distribu√≠das** s√£o **f√°ceis de construir**, por√©m **dif√≠ceis de programar**. Isso se deve ao fato de que, como cada n√∫cleo se preocupa com um recurso particular, n√£o h√° grande preocupa√ß√£o com sincroniza√ß√£o, por√©m haver√° preocupa√ß√£o por parte dos programadores para realizar acessos compartilhados de mem√≥ria, visto que isso tem que ser realizado por meio de **comunica√ß√£o expl√≠cita**.

### Programando com v√°rios n√∫cleos

Dado que **n√£o conseguimos construir m√°quinas arbitrariamente mais r√°pidas**, podemos **utilizar v√°rios processadores simples** para realizar uma tarefa de **maneira mais √°gil**. 

Al√©m disso, **processadores lentos trabalhando em conjunto realizam mais trabalho √∫til para a mesma energia gasta do que um processador mais r√°pido**.

<aside>
üí° **REALIZAR TRABALHO √öTIL** pode ser entendido como **COOPERAR PARA ATINGIR OS OBJETIVOS** (em outras palavras bem mais simples, fazer algo que preste üòÉ)

</aside>

## Fatores de Performance

$$
speedup = Tsequencial/Tparalelizado
$$

Podemos definir o ***speedup*** como sendo o ganho de tempo com a paraleliza√ß√£o de uma rotina (a raz√£o entre o tempo sequencial do problemas e o tempo paralelizado).

$$
Efici√™ncia = speedup/N
$$

Onde,

N = quantidade de n√∫cleos do processador

J√° **efici√™ncia** est√° relacionada com o qu√£o bem os recursos foram paralelizados no sistema.

Quando pensamos em paralelizar um programa, temos que ter em mente que:

- Um sistema com N processadores **dificilmente** conseguir√° aumentar seu desempenho em N
- Todo programa tem uma **parte sequencial**, mesmo que esta esteja escondida.
- Apenas a **por√ß√£o paraleliz√°vel (*f*)** ser√° **beneficiada** pelo multiprocessamento

**LEI DE AMDAHL**

A **Lei de Amdahl** √© utilizada para calcular a m√°xima velocidade esperada pra um sistema que foi **em parte paralelizado**. 

$$
speedup = 1/[(f/N) + (1 - f)]
$$

Onde, 

*f* = parte paraleliz√°vel do problema

N = n√∫mero de n√∫cleos do processador

**RESULTADOS ACERCA DA LEI DE AMDAHL**

Adicionar hardware quando a por√ß√£o paraleliz√°vel do software √© baixa pode ter **ganho n√£o significativo**. Desse modo, saber como paralelizar dentro da quantidade de recursos dispon√≠vel pode ser mais significativo. 

√â importante destacar que **a por√ß√£o de c√≥digo sequencial limita o speedup da execu√ß√£o mesmo se houver paraleliza√ß√£o**.

**CONTORNANDO A LEI DE AMDAHL**

Para contornar as conclus√µes da Lei de Amdahl, podemos pensar em algumas alternativas:

- **PROCESSADORES HETEROG√äNEOS**
    
    Ao inv√©s de termos uma quantidade N de n√∫cleos com a mesma quantidade de recursos, podemos dividir esses em **cores de diferentes capacidades**. 
    
    Processadores heterog√™neos podem suprir a limita√ß√£o do speedup nas partes sequenciais, tendo em vista que os n√∫cleos mais simples podem executar as rotinas paraleliz√°veis e as partes sequenciais serem executadas nos n√∫cleos com maior poder de processamento. 
    
- **AUMENTAR O PROBLEMA**
    
    Se potencializamos o problema em suas partes paraleliz√°veis conseguimos ter um ganho suficiente de speedup na Lei de Amdahl.
    
    Segundo a **Lei de Gustafson,** a **complexidade do problema aumenta √† medida que os recursos dispon√≠veis para sua resolu√ß√£o aumentam**. Isso √© vis√≠vel dentro da pr√≥pria evolu√ß√£o de software: com o aumento da capacidade do hardware, os softwares passaram a serem bem mais complexos e a terem uma gama de novas aplica√ß√µes, necessitando at√© de arquiteturas distribu√≠das para seu pleno funcionamento. 
    
    Pela Lei de Gustafson, podemos for√ßar que resolu√ß√µes, itera√ß√µes, c√°lculos, dentre outros aspectos, sejam alterados de modo que infle o tamanho do problema e o poder computacional aumentado seja melhor aproveitado. 
    
    Ainda dentro desse contexto, **solu√ß√µes de software menos eficientes** do ponto de vista de c√≥digo s√£o muitas vezes **melhor paraleliz√°veis**. Isso se deve ao fato de que a melhoria dos algoritmos para atingir n√≠veis de complexidade pr√≥ximos ao √≥timo s√£o realizadas para **evitar a** **repeti√ß√£o de tarefas**, o que causa grande depend√™ncia entre as estruturas de dados. Quanto menos √≥tima √© a solu√ß√£o, mais trabalho repetitivo se tem, consequentemente menos as estruturas dependem entre si e mais paraleliz√°vel √© o problema, dada esta caracter√≠stica. Portanto, aumentar a complexidade do problema pode ser vi√°vel se a alternativa √© utilizar o poder computacional inerente ao contexto. 
    

**AMDAL vs GUSTAFSON**

**AMDAL**

- Vis√£o pessimista
- Aumentar o √≠ndice de paraleliza√ß√£o n√£o √© vi√°vel se o meu problema continua sendo o mesmo

**GUSTAFSON**

- Vis√£o otimista
- Parte da premissa de que os problemas s√£o ajust√°veis e podem crescer para demandar mais processamento

## Divis√£o do trabalho

- **GRANULARIDADE**
    
    Considerando que uma rotina pode ser paralelizada em tarefas independentes (uma rotina possui parte paraleliz√°vel). A forma com que essas tarefas s√£o alocadas √† n√≠vel de gr√£o, isto √©, o tamanho que elas s√£o subdivididas, pode interferir diretamente na resolu√ß√£o do problema. Podemos compreender a granularidade como sendo a **forma que devemos dividir as rotinas paraleliz√°veis** levando em considera√ß√£o o **formato das tarefas**, a **infraestrutura dispon√≠vel** e o **tipo** da mesma, al√©m da **exist√™ncia de outras demandas externas em execu√ß√£o**. 
    
    **Gr√£os grandes**
    
    Em um ambiente homog√™neo, se as tarefas forem todas de tamanhos distintos isso pode atrasar a fase, tendo em vista que teremos a capacidade de gr√£o para complexidades diferentes de tarefas. Al√©m disso, se temos uma quantidade de cores menor que de tarefas √© evidente que de toda forma haver√° algum tipo de espera para que todas as tarefas possam ganhar algum processador e finalizar sua execu√ß√£o. O mesmo em ambientes heterog√™neos: se temos tarefas iguais executando em um processador assim√©trico, pode haver algum tipo de espera que atrase o processamento. 
    
    **Gr√£os pequenos**
    
    A espera quando se t√™m gr√£os pequenos para processamento √© bem menor, por√©m, em contrapartida, o overhead e a conten√ß√£o √© bem maior, tendo em vista a maior disputa de threads para ganhar a quantidade limitada de recursos. 
    
    **Aloca√ß√£o** 
    
    A forma com que as tarefas s√£o alocadas para tomar os recursos tamb√©m √© importante de ser levada em considera√ß√£o. Em uma implementa√ß√£o de **filas compartilhadas**, a concorr√™ncia tende a ser maior, tendo em vista que todas as tarefas s√£o vistas e disputam todos os n√∫cleos. J√° as **filas privadas** diminuiriam esse problema, tendo em vista que cada n√∫cleo teria uma fila de execu√ß√£o pr√≥pria, mas pode ocasionar o aparecimento de n√∫cleos ociosos e outros com maior conten√ß√£o. Na estrat√©gia de **work-stealing** n√∫cleo pode ‚Äúroubar‚Äù uma tarefa da fila de outro n√∫cleo se esse estiver sem tarefas e propenso a entrar em √≥cio, por√©m podem aumentar o risco de problemas de concorr√™ncia.
    
    <aside>
    üí° **RESUMINDO O RESUMO** üôÉ: Paralelizar demais as tarefas (gr√£os muito pequenos) faz com que o overhead (a sobrecarga de concorr√™ncia) seja alto e isso prejudica o sistema, dado que h√° perda de tempo na disputa de recursos. Paralelizar de menos faz com que exija mais tempo sequencial na execu√ß√£o das rotinas, o que tamb√©m pode ser mal√©fico, por√©m gr√£os maiores t√™m maior possibilidade de melhoramento de desempenho. [Linkei um material extra explicando melhor sobre isso]
    
    </aside>
    
- **LOCALIDADE**
    
    Como podemos relembrar de SO, processos e threads tendem a consumir recursos que j√° foram acessados de maneira pr√≥xima (no gr√£o de tempo e tamb√©m no de espa√ßo), a isso damos o nome de **localidade de refer√™ncia**. Para mitigar esse problema, √© bem mais barato armazenar esses dados muito acessados de maneira pr√≥xima, utilizando mem√≥rias cache, por exemplo. 
    
    Problemas de localidade podem aumentar o risco de problemas de concorr√™ncia, desse modo podemos ter algumas alternativas para mitigar esses problemas:
    
    - Duplica√ß√£o das estruturas de leitura para evitar sobrecarga de acesso
    - Agrupamento das tarefas em filas privadas
    - Aceitar resultados antigos ou aproximados (alguns sistemas que utilizam bancos de dados optam por essa estrat√©gia)
- **ESCALABILIDADE**
    
    A escalabilidade √© a qualidade de **manter um desempenho e uso dos recursos de maneira ajust√°vel**, isto √©, √† medida que o problema cresce, o desempenho para sua resolu√ß√£o tamb√©m √© mantido.
    
    Para esse aspecto, existem algumas constata√ß√µes interessantes:
    
    - Algumas **solu√ß√µes menos eficientes conseguem ser melhor paraleliz√°veis**. Desse modo, rotinas mais eficientes podem ser melhor tratadas de maneira sequencial, por√©m as menos eficientes podem ganhar essa caracter√≠stica em paraleliza√ß√£o. Imagine um cen√°rio que se deseja somar, de maneira paralela, os n√∫meros de um vetos de inteiros de tamanho N, tendo uma quantidade N/2 de cores. A solu√ß√£o mais simples para ser pensada √© realizar essa soma dois a dois, repetindo o mesmo com os resultados at√© que todos os n√∫meros sejam somados. Em primeiro momento todos os n√∫cleos estariam sendo utilizados, por√©m √† medida que a solu√ß√£o vai avan√ßando, mais n√∫cleos ficam ociosos. A complexidade dessa solu√ß√£o √© O(log n).
    - A **escolha da arquitetura** entre monol√≠tica ou multicamada pode importar para a solu√ß√£o tamb√©m. Se o problema exige uma requisi√ß√£o √∫nica, uma monol√≠tica √© perfeitamente suficiente, por√©m se crescemos isso para uma quantidade maior de requisi√ß√µes, uma arquitetura multicamada pode ser bem mais escal√°vel.
    
    √â seguro pensar sistemas de forma grande (mais exagerada) de in√≠cio e ir diminuindo a escalabilidade se necess√°rio. Al√©m disso, **aumentar a quantidade de processadores n√£o √© a resposta**!
    

## Coopera√ß√£o

Sabemos que em paralelismo podemos ter v√°rias threads e processos trabalhando juntos (e disputando recursos) para resolu√ß√£o de um problema. 

Ter esses processos e threads trabalhando em conjunto √© desafiador, tendo em vista que muitas vezes √© necess√°ria a comunica√ß√£o entre essas estruturas, tendo que manter a **sincroniza√ß√£o** e evitar interfer√™ncias.

Tamb√©m relembrando SO: podemos realizar um esquema de mem√≥ria compartilhada para isto, utilizando **buffers de mem√≥ria** para a comunica√ß√£o e **travas e sem√°foros** para realizar a **sincroniza√ß√£o**.

**ENCONTRANDO AS PARTES PARALELAS**

Para identificarmos as partes paralelas de um programa e como podemos paraleliz√°-las dentro da nossa solu√ß√£o, podemos:

‚Üí **Decompor o dom√≠nio**: subdividir um conjunto de atividades paraleliz√°veis para serem processadas nos n√∫cleos da arquitetura dispon√≠vel. 

‚Üí **Decompor a tarefa**: tendo as tarefas subdivididas, podemos encontrar ainda mais passos que podem ser paraleliz√°veis dentro dela. 

<aside>
üí° Um exemplo:
Imaginemos um cen√°rio no qual teremos que processar N imagens. Cada imagem pode ser processada de maneira independente, portanto podemos **decompor o dom√≠nio** desse problema em m√∫ltiplas threads processando imagens de maneira paralela. Por√©m, algumas regi√µes dessas imagens podem ser tamb√©m processadas de maneira paraleliz√°vel, por mais que ocorram algumas rela√ß√µes de precedencia, sendo assim, podemos tamb√©m **decompor a tarefa** em quest√£o.

</aside>

Um exemplo j√° muito visto acerca de decomposi√ß√£o do dom√≠nio √© o **pipelining**. Nessa estrat√©gia, existe uma **decomposi√ß√£o do dom√≠nio** que por sua vez subdivide-se em **decomposi√ß√£o de tarefas**, obedecendo uma ordem de prioridade.

![Untitled](assets_resumo_3/Untitled.png)

Na imagem acima, que representa uma estrat√©gia de **pipelining** aplicada √† renderiza√ß√£o de uma imagem, podemos ter as partes da imagem (vertical) divididas em **dom√≠nios** e as etapas de renderiza√ß√£o de cada uma dessas partes (horizontal), subdivididas em **tarefas**. Imagine que a renderiza√ß√£o da Imagem 3 dependa da Imagem 2, que por sua vez depende da Imagem 1 e que depende da Imagem 0. Com esse esquema √© poss√≠vel executar algumas tarefas de maneira paralela e ainda **obedecer a ordem de preced√™ncia** necess√°ria para a execu√ß√£o total da resolu√ß√£o do problema.

Alguns problemas que envolvem depend√™ncia for√ßam um processamento sequencial, em alguns deles queremos isso, como no caso dos algoritmos de criptografia: for√ßar um **processamento sequencial aumenta a complexidade** para que uma solu√ß√£o seja quebrada.

## Aplicando paralelismo e concorr√™ncia

**Modelo fork/join**

Nesse modelo, existe uma √∫nica thread ativa no in√≠cio da execu√ß√£o (**thread principal**). Essa thread, que executa por√ß√µes sequenciais, √© respons√°vel pela cria√ß√£o de outras threads (**forks**) para executar por√ß√µes paraleliz√°veis. Quando a por√ß√£o paralela termina de ser executada, as execu√ß√µes das threads se unem em um ponto da thread principal (**join**), que segue sua execu√ß√£o.  

![Untitled](assets_resumo_3/Untitled%201.png)

Como mostrado na imagem ilustrativa, o modelo fork/join para a cria√ß√£o de threads √© **muito √∫til em cen√°rios nos quais existe uma grande incid√™ncia de loops**. Cada inst√¢ncia do loop pode ser paralelizada em uma thread e, ap√≥s esperar pela execu√ß√£o total do bloco, a thread principal pode seguir a sua execu√ß√£o, realizando um join. 

Fork/join √© bastante adequado para aplica√ß√µes que cont√©m certo **balanceamento de trabalho** entre as partes paraleliz√°veis (da√≠ o uso adequado em estruras de loop). 

![Untitled](assets_resumo_3/Untitled%202.png)

```java
//Exemplo de c√≥digo com a rotina
// sequencial
public void sequencial() {
	for (i = 0; i < 100; i++) {
		executeThisFunction();
	}
}
```

√â poss√≠vel decompor o dom√≠nio dentro da estrat√©gia fork/join, tendo em vista que as threads das por√ß√µes paraleliz√°veis podem utilizar de particionamentos de recursos, como mostrado na imagem ao lado. 

Em uma aplica√ß√£o mais exemplificada na pr√°tica, podemos imaginar um c√≥digo sequencial que roda um loop, por√©m tem todas as suas itera√ß√µes independentes. Podemos subdividir essa rotina sequencial em mais de uma rotina paralela, para este caso. 

O c√≥digo ao lado poderia, por exemplo, executar em duas threads, cada qual iterando metade das itera√ß√µes totais da rotina sequencial. A escolha da divis√£o da quantidade de threads depende do problema e do contexto. [*Tentei escrever esse exemplo, mas em Java √© muito c√≥digo para escrever um c√≥digo paralelo* üòÖ]

## Sincroniza√ß√£o

**Condi√ß√µes de corrida**

Relembrando SO mais uma vez: quando duas threads ou processos acessam uma mem√≥ria ou vari√°veis compartilhadas √© bem prov√°vel que aconte√ßa uma **condi√ß√£o de corrida**. Condi√ß√µes de corrida s√£o ocasionadas por interfer√™ncias em regi√µes de mem√≥ria compartilhada, principalmente se existem threads que escrevem nessa regi√£o de mem√≥ria. As regi√µes do c√≥digo prop√≠cias para a ocorr√™ncia de condi√ß√µes de corrida s√£o chamadas de **regi√µes cr√≠ticas**.

<aside>
üí° ‚Äú*Uma condi√ß√£o de corrida ocorre quando dois threads acessam uma vari√°vel compartilhada ao mesmo tempo. O primeiro thread l√™ a vari√°vel e o segundo thread l√™ o mesmo valor da vari√°vel. Em seguida, o primeiro thread e o segundo thread executam suas opera√ß√µes no valor e eles correm para ver qual thread pode gravar o valor por √∫ltimo na vari√°vel compartilhada. O valor do thread que grava seu valor por √∫ltimo √© preservado, porque o thread est√° gravando sobre o valor que o thread anterior escreveu.‚Äù
Fonte:* [https://docs.microsoft.com/pt-BR/troubleshoot/developer/visualstudio/visual-basic/language-compilers/race-conditions-deadlocks](https://docs.microsoft.com/pt-BR/troubleshoot/developer/visualstudio/visual-basic/language-compilers/race-conditions-deadlocks)

</aside>

Alguns casos comuns que provocam condi√ß√µes de corrida s√£o **depend√™ncias entre threads e processos** (quando existe uma preced√™ncia ou depend√™ncia das sa√≠das das execu√ß√µes), **consist√™ncia de dados** (na maioria dos casos queremos que os dados escritos permane√ßam da forma com que o processo/thread escreveu, sem interfer√™ncia externas que mudem a consist√™ncia do dado) e **n√£o-determinismo de solu√ß√µes** (em casos de opera√ß√µes de ponto flutuante, por exemplo, as aproxima√ß√µes e arredondamentos podem deixar os dados n√£o determin√≠sticos dada a ordem das opera√ß√µes realizadas).

Para resolver o problema da condi√ß√£o de corrida, podemos utilizar estrat√©gias de **exclus√£o m√∫tua** em regi√µes cr√≠ticas.

**Exclus√£o m√∫tua**

A exclus√£o m√∫tua pode ser implementada de diversas maneiras, algumas mais eficientes que outras, por√©m sempre devem apresentar as seguintes caracter√≠sticas:

‚Üí **Safety**: a solu√ß√£o √© segura e correta

‚Üí **Liveness**: algo de positivo para a solu√ß√£o acontece

 ****Al√©m disso √© importante que a implementa√ß√£o n√£o cause nenhuma das condi√ß√µes abaixo:

‚Üí **Deadlock**: todos os processos envolvidos n√£o conseguem progredir por estarem esperando pela execu√ß√£o um dos outros.

‚Üí **Starvation**: um processo nunca consegue tomar a CPU

**Implementando exclus√£o m√∫tua** 

√â importante relembrar que **processos s√£o estruturas independentes** e, apesar de compartilharem alguns recursos, **threads t√™m execu√ß√£o independente**. Desse modo, **threads/processos n√£o s√£o capazes de observar a execu√ß√£o dos demais**. Para que esses fluxos de execu√ß√£o sejam sincronizados, √© necess√°ria uma comunica√ß√£o expl√≠cita. 

**‚ÄúProtocolo do celular‚Äù** 

Nessa analogia, uma thread ‚Äúligaria‚Äù para a outra na tentativa de saber o seu status de acesso √† regi√£o cr√≠tica. Por√©m, a comunica√ß√£o nunca √© garantida para esses casos, o que faria com que a propriedade de **liveness** n√£o fosse atendida. Al√©m disso, como uma thread estaria sempre esperando pela resposta da outra para tomar a regi√£o cr√≠tica, √© f√°cil ocorrer um **deadlock**.

A comunica√ß√£o entre os fluxos envolvidos deve ser persistente, pense em algo como uma comunica√ß√£o por escrita.

**‚ÄúProtocolo da lata‚Äù**

J√° nesta analogia, cada fluxo teria uma quantidade de fichas (ou latas) para sinalizar que est√° acessando a regi√£o cr√≠tica. Sempre que um fluxo est√° para atingir a regi√£o cr√≠tica, este derruba uma lata em sinaliza√ß√£o, fazendo o mesmo quando sair. Dessa forma, os demais fluxos podem observar se uma lata daquele fluxo est√° ca√≠da em posi√ß√£o par, indicando que a regi√£o cr√≠tica est√° em acesso, e aguardar para fazer sua entrada na regi√£o cr√≠tica quando uma lata de posi√ß√£o √≠mpar estives ca√≠da, indicando a sa√≠da. [*Considere uma indexa√ß√£o iniciando em 0 e indo at√© um √≠ndice N*]. 

O maior impasse dessa solu√ß√£o √© que a quantidade de latas/fichas √© finita. Em uma aplica√ß√£o real, o ‚Äúprotocolo da lata‚Äù tenta resolver o problema de condi√ß√£o de corrida com uma interrup√ß√£o, mas isso n√£o √© poss√≠vel, tendo em vista que precisar√≠amos de um conjunto ilimitado de latas para que a solu√ß√£o fosse totalmente generalizada. 

**Protocolo da bandeira: Flags**

Uma forma mais efetiva de realizar exclus√£o m√∫tua seria sinalizar de uma maneira mais clara que a regi√£o cr√≠tica est√° em acesso. Para isso, a thread ou processo que acessa regi√£o pode **levantar uma bandeira** indicando que est√° executando aquele c√≥digo. √Ä n√≠vel de implementa√ß√£o, uma **flag** pode ser entendida como uma vari√°vel compartilhada, uma para cada fluxo, na qual o mesmo seta o bit 1 quando ocupa a regi√£o cr√≠tica e o bit 0 quando sai dela. Desse modo, √© poss√≠vel que um fluxo de execu√ß√£o ‚Äúlevante a bandeira‚Äù, indicando que quer tomar a regi√£o cr√≠tica, e observe a flag do outro no momento de entrar, sendo assim, se o outro fluxo estiver com flag 0, a regi√£o est√° livre para ser acessada.

Por√©m, podem existir casos nos quais os dois fluxos levantam suas bandeiras no mesmo instante de tempo. Nesse caso, podemos pensar em uma abordagem de prioridade para resolver esse problema.

Imagine a exist√™ncia da **thread A** e da **thread B** que concorrem pela mesma regi√£o cr√≠tica. Se A e B levantam suas bandeiras ao mesmo tempo, gentilmente **B pode ceder o seu lugar para A**, e isso acontece sempre que ambas as bandeiras levantam ao mesmo tempo. √â uma solu√ß√£o que inibe a condi√ß√£o de corrida, por√©m causa **starvation** para um dos fluxos, nesse caso a thread B. No pior caso, no qual as duas bandeiras s√£o levantadas ao mesmo tempo sempre, a thread A sempre ganha a regi√£o cr√≠tica e a thread B nunca consegue seu lugar.  

Essa solu√ß√£o n√£o apresenta **deadlocks**, dado que um dos fluxos sempre ganha a regi√£o, por√©m **n√£o √© justo**, tendo em vista a grande probabilidade de **starvation**. 

**Algoritmos para implementa√ß√£o de exclus√£o m√∫tua**

Na pr√°tica, as linguagens de programa√ß√£o j√° implementam solu√ß√µes para exclus√£o m√∫tua, portanto, n√£o precisaremos codific√°-las, por√©m √© importante que haja o entendimento do funcionamento. 

**Algumas defini√ß√µes**

**Eventos** s√£o acontecimentos no hist√≥rico de uma thread e **ocorrem unicamente em um instante**.

Podemos entender uma **thread** como sendo uma **sequ√™ncia de eventos**. Al√©m disso, **threads s√£o m√°quinas de estados**, um evento pode transicionar para outro ou ser repetido em loop, por exemplo. 

Duas ou mais threads se **entrela√ßam** se s√£o executadas no mesmo instante de tempo, tendo seus eventos ocorrendo na soma dos instantes de tempo.

Um **intervalo** √© o **instante de tempo entre dois eventos**. Intervalos de duas ou mais threads podem se sobrepor ou n√£o, isto √©, a execu√ß√£o de dois ou mais eventos pode ser simult√¢nea ou sequencial. 

Podemos dar **preced√™ncia** ao acontecimento dos eventos. Nota√ß√£o:

A ‚Üí B, se o evento inicial de A ocorreu antes do evento inicial de B.

Algumas conclus√µes acerca disso:

- A ‚Üí A nunca √© poss√≠vel (irreflexiva)
- Se A ‚Üí B ent√£o √© imposs√≠vel que B ‚Üí A (antissim√©trica)
- Se A ‚Üí B e B ‚Üí C, ent√£o A ‚Üí C (transitiva)

Essa propriedade pode ser **parcial**, quando ocorre **concorr√™ncia** ou sem empates, quando h√° preced√™ncia total dos eventos. 

**Locks (travas)**

Uma trava √© uma estrutura que impede que uma thread entre em uma regi√£o cr√≠tica. Abaixo est√° um exemplo de como a linguagem de programa√ß√£o Java lida com travas:

```java
private Lock lock;
private int value;
public void lockExample() {
	lock.lock(); // Trava na regi√£o cr√≠tica
	int temporary;
	try {
		temporary = value; //Regi√£o cr√≠tica dentro do bloco try
		incrementValue(value)
	} finally {
		lock.unlock() //Destrava quando a execu√ß√£o da thread sai da regi√£o cr√≠tica
	}
	return temporary;
}
```

No exemplo acima, a rotina **sempre √© destravada** quando sai da regi√£o cr√≠tica. Podemos dessa forma garantir que n√£o h√° **starvation** para essa solu√ß√£o, tendo em vista que todas as threads poder√£o ganhar o acesso da regi√£o cr√≠tica, assim como n√£o h√° **deadlocks**, tendo em vista que o sistema consegue caminhar em sua execu√ß√£o (por mais que algumas threads estejam em espera).

Os exemplos antes vistos para **flags** podem ser implementados utilizando travas, de modo que uma trava √© ativada sempre que l√≥gica das bandeiras √© seguida. 

## **Implementando travas**

**Algoritmo de Peterson**

O algoritmo de Peterson implementa uma trava com exclus√£o m√∫tua por **espera ocupada**. Nessa categoria de implementa√ß√£o, quando um processo ganha a CPU, os demais processos ficam ocupados com alguma barreira at√© que o processo/thread ocupante deixe a regi√£o. 

Abaixo est√° uma implementa√ß√£o em Java comentada do algoritmo de Peterson:

```java
public void lock() {
	//Levantando a bandeira da thread i para que ela sinalize que 
	//quer ganhar a regi√£o cr√≠tica
	flag[i] = true 
	//Por hora, a minha vez √© cedida para outra thread
	victim = i
	
	//Se a thread j est√° na regi√£o cr√≠tica e eu (thread i) sou a v√≠tima,
	//ent√£o me ocupo com alguma rotina at√© que possa adentrar a regi√£o
	while (flag[j] && victim == i) {}; 
}

public void unlock() {
	//Abaixa a bandeira da thread sinalizando que ela saiu da regi√£o cr√≠tica
	flag[i] = false
}
```

Em uma rotina, chamar√≠amos lock() antes da regi√£o cr√≠tica e unlock() ap√≥s a mesma.

A solu√ß√£o de Peterson **implementa a exclus√£o m√∫tua**. Podemos conferir essa propriedade de maneira simples e dedutiva:

Em um certo estado do programa temos que a **Thread 0** (i = 0) e a **Thread 1** (j = 1) est√£o concorrendo pelos recursos de uma regi√£o cr√≠tica.

Quando a **Thread 0** est√° na regi√£o cr√≠tica:

flag[0] = true

victim = 0

Isso quer dizer que a Thread 0 quer ganhar a regi√£o cr√≠tica, mas se caso a Thread 1 tamb√©m queira (ou j√° esteja nela), a Thread 0 cede a vez at√© que a Thread 1 baixe a bandeira.

Quando a **Thread 1** est√° na regi√£o cr√≠tica:

flag[1] = true

victim = 1

Isso quer dizer que a Thread 1 quer ganhar a regi√£o cr√≠tica, mas se caso a Thread 0 tamb√©m queira (ou j√° esteja nela), a Thread 1 cede a vez at√© que a Thread 0 baixe a bandeira.

Perceba que os dois estados n√£o podem ser verdadeiros ao mesmo tempo, dessa maneira, √© poss√≠vel habilitar que apenas uma thread esteja acessando a regi√£o cr√≠tica por vez.

O algoritmo de Peterson tamb√©m √© **livre de deadlocks**, isto √©, as threads n√£o ficam sempre esperando uma pela outra. Pelos mesmos motivos acima, podemos assegurar que:

- Uma thread sempre ser√° v√≠tima e outra n√£o, mas nunca as duas estar√£o ocupando o mesmo estado, isto √©, ambas n√£o sendo v√≠tima ou sendo v√≠tima.
- Uma thread bloqueia apenas quando ela √© a v√≠tima e a outra est√° ocupando a regi√£o cr√≠tica.

Por fim, a solu√ß√£o tamb√©m **livre de starvation**. Por mais que haja espera de uma pela outra, podemos garantir que todas as threads envolvidas sempre ir√£o ganhar a regi√£o cr√≠tica em algum momento. Como as condi√ß√µes de estado das vari√°veis s√£o sempre contr√°rias (se uma √© verdadeira, a outra √© falsa), n√£o haver√° espera infinita por parte de nenhuma das threads envolvidas, dado que a thread que toma, sempre sai da regi√£o cr√≠tica. 

Em resumo, o **Algoritmo de Peterson**:

‚Üí Cede a vez para executar a regi√£o cr√≠tica, mas apenas se a outra thread quiser tom√°-la.

‚Üí √â livre de deadlocks.

‚Üí √â livre de starvation.

‚Üí Implementa exclus√£o m√∫tua.

‚Üí √â aplic√°vel somente para casos bin√°rios (com duas threads envolvidas). 

**Algoritmo Bakery**

Para sanar as limita√ß√µes do algoritmo de Peterson, principalmente relacionado ao fato de que Peterson s√≥ suporta duas threads em sua implementa√ß√£o, podemos implementar o **algoritmo da padaria** ou **Bakery**.

O bakery **suporta mais de uma thread** em sua implementa√ß√£o e atende uma l√≥gica **baseada em FCFS (First Come First Served)**. 

A ideia principal √© que, cada thread pegue um n√∫mero (uma ficha, no contexto da padaria üôÇ) e espere at√© que este seja decrementado at√© que seja o menor. Dessa forma, o menor n√∫mero √© o primeiro que chegou e ser√° o primeiro a ser atendido, prosseguindo, sempre que a fila da padaria anda, o n√∫mero dos demais clientes esperando na fila √© decrementado, at√© que novos n√∫meros menores surgir√£o e ser√£o atendidos. 

Implementando essa l√≥gica em Java:

```java
public void lock(int id) {
	//Dessa vez, recebemos um id, que √© a ficha de atendimento da nossa thread
	//Quando o cliente chega na fila da padaria, ele n√£o tem ficha, portanto 
	//seu n√∫mero √© 0.

	for (int j = 0;  j < n; j ++) { //Visitando todos os clientes da fila...
		if (num[j] > num[id]) { //Se existirem pessoas com fichas maiores...
			num[id] = num[j]; //Eu recebo essa ficha
		}		
		num[id] ++; //At√© que chegue no final da fila e receba uma ficha exclusiva
								//(A √∫ltima posi√ß√£o incrementada de 1)
	}

	
	for (int j = 0;  j < n; j ++) { //Visitando todos os clientes da fila...
		while((num[j] != 0) && (num[j] < num[id])); //Enquanto n√£o chegar a vez, espere
	}
}

public void unlock(int id) {
	num[id] = 0; //O cliente saiu da padaria, se ele voltar, tem que entrar
							//novamente na fila	
}
```

Essa solu√ß√£o de implementa√ß√£o parece funcionar, mas apresenta um problema:

Numa aplica√ß√£o real, uma thread pode ter rapidamente passado pelo for do contador e perdido o processador, desse modo, uma outra thread pode ter o mesmo contador que ela, causando um conflito da solu√ß√£o. 

Uma solu√ß√£o para esse problema seria utilizar flags para indicar que se est√° escolhendo uma ficha para a thread que solicitou o lock:

```java
public void lock(int id) {
	//Dessa vez, recebemos um id, que √© a ficha de atendimento da nossa thread
	//Quando o cliente chega na fila da padaria, ele n√£o tem ficha, portanto 
	//seu n√∫mero √© 0.
	choosing[id] = true; //Sinalizamos para as demais que estamos escolhendo
	for (int j = 0;  j < n; j ++) { //Visitando todos os clientes da fila...
		if (num[j] > num[id]) { //Se existirem pessoas com fichas maiores...
			num[id] = num[j]; //Eu recebo essa ficha
		}		
		num[id] ++; //At√© que chegue no final da fila e receba uma ficha exclusiva
								//(A √∫ltima posi√ß√£o incrementada de 1)
	}
	choosing[id] = false; //A ficha foi escolhida e o cliente est√° na fila
	
	for (int j = 0;  j < n; j ++) { //Visitando todos os clientes da fila...
		//Enquanto n√£o chegar a vez, espere	
		while(choosing[j]);
		while((num[j] != 0) && ((num[j] < num[id]) || 
				((num[id] == num[j]) && j < id))); 
	}
}

public void unlock(int id) {
	num[id] = 0; //O cliente saiu da padaria, se ele voltar, tem que entrar
							//novamente na fila	
}
```

Assim como a solu√ß√£o de Peterson, o algoritmo da padaria **√© livre de deadlocks e de starvation**, al√©m de tamb√©m implementar a exclus√£o m√∫tua (e por espera ocupada).  

Essa √∫ltima solu√ß√£o proposta funciona, por√©m, em sistemas em que existem muitas threads ou que a arquitetura para representa√ß√£o √© limitada, essa solu√ß√£o pode ocasionar um **overflow**, dado que sempre h√° o incremento de um n√∫mero. 

Uma forma de mitigar esse problema √©, ao inv√©s de utilizar fichas num√©ricas, utilizar flags por posi√ß√£o.

**Pontos negativos da exclus√£o m√∫tua**

- **Interrup√ß√µes ass√≠ncronas**
    
    A ocorr√™ncia de interrup√ß√µes de maneira ass√≠ncrona podem prejudicar o funcionamento das demais threads que querem tomar a regi√£o cr√≠tica.
    
- Processadores heterog√™neos
    
    Tempos de clock diferentes, tendo em vista o contexto de processadores heterog√™neos, podem dificultar a sincroniza√ß√£o
    
- Toler√¢ncia √† falhas
    
    A toler√¢ncia √† falha da exclus√£o m√∫tua n√£o √© t√£o desej√°vel para certos tipos de problema.
    

### Opera√ß√µes at√¥micas

Opera√ß√µes at√¥micas s√£o opera√ß√µes **semanticamente compostas**, mas que s√£o **realizadas de forma at√¥mica** pelo processador.

<aside>
üí° Em outras palavras, s√£o pequenos blocos de c√≥digo que conseguem operar em um pequeno gr√£o de sincroniza√ß√£o.

</aside>

Essas opera√ß√µes s√£o **n√£o bloqueantes**, tendo em vista que s√£o **executadas em um mesmo gr√£o** e **evitam deadlocks**, com **toler√¢ncia maior √† falhas**. 

<aside>
‚ö†Ô∏è As **opera√ß√µes at√¥micas** s√£o **n√£o bloqueantes**, mas isso n√£o impede que uma rotina que utiliza de uma opera√ß√£o at√¥mica seja bloqueante.

</aside>

Opera√ß√µes at√¥micas s√£o um √≥timo mecanismo para **implementar blocos b√°sicos e sistemas que exigem alto desempenho**.

O desempenho de uma opera√ß√£o at√¥mica √© sem d√∫vida seu diferencial, mas a constru√ß√£o de algoritmos que sejam totalmente livres de bloqueio √© ainda um desafio, dado que estamos trabalhando ainda com concorr√™ncia. Por exemplo, uma opera√ß√£o pode manipular uma vari√°vel de maneira at√¥mica, por√©m quando isso acontece de maneira simult√¢nea, perdemos essa propriedade. 

A sem√¢ntica da implementa√ß√£o de uma opera√ß√£o at√¥mica, em Java, est√° disposta abaixo:

```java
//Exemplo de opera√ß√£o at√¥mica compare and swap
public synchronized int compareAndSwap(int expected, int newV) {
	int oldValue = value;
	//Comparando se o valor antigo √© o esperado, caso seja, troca o valor atual
	//pelo valor novo
	if (oldValue == expected) value = newV; 
	return oldValue; // Retorna o antigo valor
}
```

Podemos perceber no exemplo de implementa√ß√£o acima que h√° a execu√ß√£o de toda a l√≥gica de maneira **indivis√≠vel**.

Abaixo est√° um segundo exemplo, com uma fun√ß√£o de incremento, representando a regi√£o cr√≠tica de uma solu√ß√£o paralela e concorrente que utiliza a solu√ß√£o de compare and swap acima:

```java
public int increment() {
	int v;
	// Enquanto o valor retornado for diferente do atual, n√£o houve incremento...
  // Tente novamente...
	do {
		v = value.get() // Seta a vari√°vel com o valor atual
	} while(v != value.compareAndSwap(v, v+1));

	return v + 1
}

```

Opera√ß√µes at√¥micas tamb√©m t√™m a propriedade de serem **lock-free**, isto √©, **threads que est√£o paradas** (por exemplo, por ventura perderam o processador), **n√£o atrapalham o funcionamento das demais**. Al√©m disso, tamb√©m s√£o **wait-free**, ou seja, as threads n√£o s√£o atrapalhadas por outras que est√£o fazendo progresso. 

### Monitores

Em Java, todo objeto herda da classe padr√£o Object uma **trava de monitor**. A forma mais simples de utilizar essa trava do ponto de vista que ela foi implementada por padr√£o √© utilizando a marca√ß√£o **synchronized**. 

Um monitor permite que ocorra **exclus√£o m√∫tua** e **monitoramento sem espera ocupada**. Essa melhoria √© bastante significante, tendo em vista que a espera ocupada apresenta uma grande gama de gasto energ√©tico e de processamento (espera ocupada queima CPU!).

Na implementa√ß√£o de um monitor, a thread que n√£o consegue atingir a regi√£o cr√≠tica pode temporariamente perder a CPU e ‚Äúdormir‚Äù, de modo que poder√° ser acordada quando a thread corrente na regi√£o cr√≠tica sair. Em Java, s√£o tr√™s as estruturas de implementa√ß√£o not√°veis para realizar a sincroniza√ß√£o com uso de monitores:

‚Üí wait() : inicia a espera (lock!)

‚Üínotify() : notifica uma thread de que a regi√£o cr√≠tica est√° livre (acorda a thread!) 

‚ÜínotifyAll() : notifica todas as threads de que a regi√£o cr√≠tica foi liberada.

√â importante lembrar que **toda inst√¢ncia possui sua trava**, mas tamb√©m que **a classe associada possui outra**. O mal uso e n√£o conhecimento dessas duas estruturas podem causar **conten√ß√£o**. 

Outra propriedade not√°vel dos monitores √© que sua trava √© **reentrante**, isto √©, a trava possui um ‚Äúdono‚Äù que pode adentrar a regi√£o cr√≠tica diversar vezes quando quiser. Esse tipo de estrutura √© bastante √∫til para travamentos ocorridos em **estruturas recursivas**, tendo em vista que se n√£o houver reentr√¢ncia nessas situa√ß√µes haver√° o **ocasionamento de um deadlock**. 

**TRAVAS vs MONITORES**

**TRAVAS**

Travas permitem a defini√ß√£o de regi√µes cr√≠ticas menores, al√©m de, dada a menor abstra√ß√£o de sua implementa√ß√£o, favorecem funcionalidades mais avan√ßadas, como timeout, justi√ßa, interrup√ß√µes, dentre outros aspectos

**MONITORES**

Como os monitores s√£o implementa√ß√µes pr√≥prias das linguagens de programa√ß√£o, o seu n√≠vel de abstra√ß√£o tamb√©m √© maior. Tendo isso em vista, s√£o menos configur√°veis e podem favorecer mal uso, mas existe a facilidade de utiliza√ß√£o com o destravamento autom√°tico das regi√µes cr√≠ticas e implementa√ß√£o sem uso de espera ocupada.