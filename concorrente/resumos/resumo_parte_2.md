# RESUMO PROGRAMA√á√ÉO CONCORRENTE - PARTE 2

**LINKS √öTEIS E MATERIAIS EXTRA**

*Alguns desses materiais est√£o dispostos durante este resumo para consulta complementar.*

**BARREIRAS**

[Barreiras](http://www.lac.inpe.br/~celso/cap334/aula8/aula8d/tsld016.htm)

[Barrier Synchronization - Georgia Tech - Advanced Operating Systems](https://www.youtube.com/watch?v=fCFndi19yNs)

**WRITE BACK**

[Write Back Cache Example - Georgia Tech HPCA Part 3](https://www.youtube.com/watch?v=xU0ICkgTLTo)

**TRAVA DE ANDERSON (TRAVAS BASEADAS EM ARRAY)**

[Array Based Queueing Lock - Georgia Tech - Advanced Operating Systems](https://www.youtube.com/watch?v=yKsH-QGtXPU)

[Array Based Queueing Lock (cont) - Georgia Tech - Advanced Operating Systems](https://www.youtube.com/watch?v=W6rrkR2v2G8)

[Array Based Queueing Lock (cont) - Georgia Tech - Advanced Operating Systems](https://www.youtube.com/watch?v=ur9vyAbYb2Q)

**TRAVA CLH**

[Computa√ß√£o concorrente, cap. 7, parte 7: Spin locks (CLH locks)](https://www.youtube.com/watch?v=t0SflcbE6WA)

**TRAVA MCS**

[Link Based Queueing Lock - Georgia Tech - Advanced Operating Systems](https://www.youtube.com/watch?v=R-xJmv2cp5Q)

[Link Based Queueing Lock (cont) - Georgia Tech - Advanced Operating Systems](https://www.youtube.com/watch?v=SJ3Ys9n2dgM)

[Link Based Queueing Lock (cont) - Georgia Tech - Advanced Operating Systems](https://www.youtube.com/watch?v=o87PPY3MSvg)

[Link Based Queueing Lock (cont) - Georgia Tech - Advanced Operating Systems](https://www.youtube.com/watch?v=SXUJ40PtcRo)

[Link Based Queueing Lock (cont) - Georgia Tech - Advanced Operating Systems](https://www.youtube.com/watch?v=Da5xO_eQ-Qg)

[Link Based Queueing Lock (cont) - Georgia Tech - Advanced Operating Systems](https://www.youtube.com/watch?v=P3dFwI_wQpc)

[Computa√ß√£o concorrente, cap. 7, parte 8: Spin locks (MCS locks)](https://www.youtube.com/watch?v=7J6RfQcIoF8&t=1s)

**MODELOS DE CONSIST√äNCIA** 

[[FSPD] 12b: modelos de consist√™ncia centrados nos dados](https://www.youtube.com/watch?v=xal3WOz3b2k)

**TIPOS DE SINCRONIZA√á√ÉO**

[Computa√ß√£o concorrente, cap. 9, parte 1: sincroniza√ß√£o grossa e fina (coarse/fine grained sync)](https://www.youtube.com/watch?v=5go096Rzi40&list=PLRI_8pjQtk5Q03TTNhpZuGpeIbuBQmt3f&index=15)

[Computa√ß√£o conc., cap. 9, parte 2: sincr. otimista (optimistic sync) e sincr. pregui√ßosa (lazy sync)](https://www.youtube.com/watch?v=DydGnF5mzqQ&list=PLRI_8pjQtk5Q03TTNhpZuGpeIbuBQmt3f&index=16)

[Computa√ß√£o concorrente, cap. 9, parte 3: solu√ß√µes n√£o-bloqueantes](https://www.youtube.com/watch?v=Ra4UZQQoMfM&list=PLRI_8pjQtk5Q03TTNhpZuGpeIbuBQmt3f&index=17)

# Barreiras

**DEFINI√á√ÉO**

Uma barreira √© um **m√©todo de sincroniza√ß√£o** no qual nenhuma thread pode ir adiante na sua execu√ß√£o at√© que **todas** as demais **tenham atingido um determinado ponto de execu√ß√£o**. 

![KBJ9G60H38KQ0S73.gif](assets_resumo_2/KBJ9G60H38KQ0S73.gif)

**UTILIDADE** 

Barreiras s√£o implementa√ß√µes √∫teis de sincroniza√ß√£o quando o contexto exige **sincroniza√ß√£o coletiva**, isto √©, que todas as threas tenham atingido um determinado ponto para que possam seguir adiante.

**IMPLEMENTA√á√ÉO**

Barreiras s√£o implementadas com **dois spinlocks**: um primeiro para proteger o contator de chegadas √† barreira e o outro que indica que todas as threads atingiram o ponto de sincroniza√ß√£o e podem seguir.  ****O primeiro spinlock pode ser evitado se a implementa√ß√£o utilizar opera√ß√µes at√¥micas para decrementar o contador.

<aside>
‚ö†Ô∏è **N√ÉO CONFUNDIR**

Barreiras (*barriers*) n√£o s√£o o mesmo que ‚Äúcercas‚Äù (*fences*)*.* Barreiras est√£o melhor relacionadas com sincroniza√ß√£o de execu√ß√£o, enquanto *fences* est√£o relacionadas √† consist√™ncia de mem√≥ria.

</aside>

## **ESTUDO DE CASO**

<aside>
üí° Considere o problema de **renderiza√ß√£o de um v√≠deo**, no qual o dom√≠nio √© subdividido em tarefas que compreendem preparar e mostrar um frame.

</aside>

### **M√öLTIPLAS THREADS TRABALHANDO NO PROBLEMA**

```java
int threadID = TreadID.get()

while (true) {
	frame[treadID].prepare();
	frame[treadID].display();
} 
```

**QUAL O PROBLEMA DESSA SOLU√á√ÉO?** 

Algumas threads podem ser mais lentas que outras por uma por√ß√£o de motivos, como a dificuldade/complexidade de processamento de um frame ou a perda do processador por parte dessa thread (um cen√°rio em que o n√∫mero de threads √© maior que o n√∫mero de processadores dispon√≠veis), fatores que podem diminuir o desempenho de tempo desses fluxos de execu√ß√£o.

Desse modo, podemos ter threads que renderizem frames mais rapidamente que outras e isso poder√° atrasar a resolu√ß√£o do problema.

### UMA PREPARA, OUTRA EXIBE

Podemos tamb√©m pensar em uma simplifica√ß√£o bin√°ria, na qual enquanto uma thread prepara um frame, a outra exibe e assim por diante. Essa implementa√ß√£o pode ser realizada via uma flag:

```java
while (true) {
	if (turn) {
		frame[0].display();
	} else {
		frame[1].display();
	}
	
	turn = !turn
}
```

```java
while (true) {
	if (turn) {
		frame[1].prepare();
	} else {
		frame[0].prepare();	
	}

	turn = !turn
}
```

Mas como iremos garantir sincronia nesses casos?

### UTILIZAR UMA SOLU√á√ÉO COM BARREIRAS

```java
private Barrier barrier

while (true) {
	frame[threadID].prepare();
	barrier.await() // Espera que todas as threads atinjam esse ponto
	frame[threadID].display();
	barrier.await() // Novamente aguarda todas as threads atingirem esse ponto
}
```

No exemplo acima h√° um uso simplificado de uma barreira para realizar o preparo e a exibi√ß√£o dos frames, no qual todas as threads sincronizam ap√≥s finalizarem a prepara√ß√£o e em seguida continuam suas execu√ß√µes mostrando o conte√∫do, sincronizando novamente ap√≥s isso e assim por diante. Mas, como s√£o implementadas essas barreiras?

## Implementando Barreiras

**BARREIRAS COM VARI√ÅVEIS AT√îMICAS E CONTADORES**

Uma **barreira com contador** √© implementada de modo que um **contador** √© inicializado no in√≠cio do bloco de sincroniza√ß√£o com **a quantidade de threads** que se deseja sincronizar pela barreira. 

A cada thread que atinge o ponto de sincroniza√ß√£o, um **decremento at√¥mico** √© realizado do contador, at√© que este seja zerado.

Se a thread que decrementou **n√£o zerou** o contador, isso significa que existe alguma thread que ainda **n√£o atingiu a barreira**, portanto esta ficar√° em **spin (espera ocupada)** at√© que todas as threads **sincronizem no ponto desejado** e possam seguir suas execu√ß√µes.

![OP2J9UI2O7LPJFED (1).gif](assets_resumo_2/OP2J9UI2O7LPJFED_(1).gif)

[‚Üí V√çDEO RESUMIDO SOBRE BARREIRAS](https://www.notion.so/assets_resumo_2-575677e675fe4395bcf951990f26360e) 

# Implementa√ß√£o de componentes concorrentes

Quando estamos implementando uma solu√ß√£o concorrente, devemos estar cientes de que existem alguns **compromissos** e **crit√©rios de corretude** a depender do problema. 

## Spinlocks e conten√ß√£o

**TROCA DE CONTEXTO (*BLOCK*) OU ESPERA OCUPADA (*SPIN*)?**

Como dito anteriormente, a depender do contexto pode ser mais eficiente escolher uma ou outra estrat√©gia para realiza√ß√£o da sincroniza√ß√£o. Mas, qual √© melhor diante do contexto?

**ESPERA OCUPADA (SPIN)**

A espera ocupada pode ser eficiente se o contexto apresenta **regi√µes cr√≠ticas pequenas**. Isso se apresenta pelo fato de que realizar o spin √©, em outras palavras, ‚Äúesquentar‚Äù a CPU com uma atividade que n√£o √© √∫til no contexto, mantendo o processador ocupado e n√£o utiliz√°vel por outros processos nesse per√≠odo. Em caso de aplicado a regi√µes cr√≠ticas longas, a espera ocupada pode atrasar a execu√ß√£o de outras tarefas, por n√£o haver libera√ß√£o do uso dos recursos durante aquele espa√ßo de tempo. J√° em contextos com regi√µes cr√≠ticas pequenas, vale mais manter uma espera do que realizar uma troca de contexto com opera√ß√µes em mem√≥ria, que s√£o bastante custosas do ponto de vista operacional. 

**TROCA DE CONTEXTO (BLOCK)**

A troca de contexto muitas vezes se mostra custosa pela sua necessidade de ler e escrever da mem√≥ria (opera√ß√µes de alto custo). Por√©m, em se tratando de **regi√µes cr√≠ticas longas**, troca de contexto pode ser menos custosa do que manter uma espera ocupada por tempo consider√°vel. J√° em contextos contr√°rios, realizar block em regi√µes cr√≠ticas muito curtas pode se mostrar bastante custoso, tendo em vista a grande quantidade de opera√ß√µes em mem√≥ria que seriam requeridas em um pequeno espa√ßo de tempo. 

Em sistemas **uniprocessados**, essa √© a √∫nica forma de sincroniza√ß√£o que faz algum sentido, dado que manter uma espera ocupada iria tomar toda a capacidade de processamento do mesmo. 

**TRAVANDO COM OPERA√á√ïES AT√îMICAS**

Uma **opera√ß√£o at√¥mica** realiza toda a sua execu√ß√£o de maneira **indivis√≠vel**, isto √©, n√£o √© poss√≠vel haver regi√£o cr√≠tica durante a execu√ß√£o de sua opera√ß√£o. Essas opera√ß√µes s√£o implementadas em um n√≠vel de software muito mais baixo, bem pr√≥ximo ao processador, por meio de instru√ß√µes (como a famosa **Test And Set**). 

O benef√≠cio de utilizar um m√©todo de sincroniza√ß√£o implementado por uma opera√ß√£o at√¥mica √© que existe **simplicidade** e **ocupa pouca mem√≥ria**, podendo utilizar de apenas uma regi√£o para uma implementa√ß√£o que exige a sincronia de N threads. 

Por√©m, quando o contexto exige muitos travamentos e destravamentos, o tempo para a execu√ß√£o do algoritmo **n√£o se mant√©m constante**, mas sim praticamente exponencial, isto √©, quanto mais threads se t√™m para sincronizar, menos eficiente √© utilizar uma opera√ß√£o at√¥mica para tal.  

Esse ganho de complexidade est√° muito relacionado ao meio f√≠sico do sistema computacional, pois barramentos podem ser sobrecarregados com acessos √† mem√≥ria e aumentar a complexidade do m√©todo. Como podemos contornar esse tipo de problema?

**ARQUITETURAS BASEADA EM BARRAMENTOS**

Quando estamos tratando de m√°quinas pequenas, v√°rios processadores acessam a mem√≥ria atrav√©s do mesmo barramento.

Muitos acessos √† mem√≥ria podem ocasionar sobrecarca dos barramentos desse sistema computacional.

Uma das formas de contornar esse tipo de ocorr√™ncia √© aproximando o dado para uma mem√≥ria de mais f√°cil acesso. Uma op√ß√£o trivial √© utilizar mem√≥rias cache.

Cada processador pode ter um **cache local** que prioriza os acessos dos dados ali armazenados.

![Captura de tela de 2022-07-21 17-08-08.png](assets_resumo_2/Captura_de_tela_de_2022-07-21_17-08-08.png)

Desse modo, ao inv√©s de sempre buscar na mem√≥ria mais distante, cada processador pode buscar o dado desejado em sua mem√≥ria pr√≥xima. Assim, existe uma diminui√ß√£o do fluxo de dados no barramento compartilhado entre essas entidades. 

Al√©m disso, outros processadores podem observar dados dos caches locais de outros e diminuir o fluxo no barramento principal quando se tenta buscar o dado da mem√≥ria mais distante. Por√©m, algumas medidas devem ser tomadas relativas √† inconsist√™ncia de dados, dado que, como cada processador pode alterar o seu dado de cache local, um outro processador pode observar um dado inconsistente com o da mem√≥ria. Para isso, podemos adotar algumas estrat√©gias, sendo a mais utilizada o **write back**.

**WRITE BACK**

O write back √© uma estrat√©gia de escrita e leitura em cache que leva em considera√ß√£o medidas para manter a consist√™ncia de um dado. Nessa alternativa, podemos considerar que um dado pode assumir tr√™s estados:

‚Üí V√ÅLIDO: quando est√° em concord√¢ncia com o que foi lido da mem√≥ria

‚Üí INV√ÅLIDO: quando a c√≥pia em cache est√° inconsistente (houve uma mudan√ßa no dado da mem√≥ria)

‚Üí SUJO: quando a c√≥pia tem valor alterado

Nesse caso, s√≥ √© necess√°rio escrever o dado na mem√≥ria quando houver a leitura de um dado em cache que foi sujo, isto √©, localmente alterado.

![M2V5VN909F774S03 (1).gif](assets_resumo_2/M2V5VN909F774S03_(1).gif)

[‚Üí V√çDEO RESUMIDO SOBRE WRITE BACK](https://www.notion.so/assets_resumo_2-575677e675fe4395bcf951990f26360e)

**WRITE BACK RESOLVE O GARGALO DO TEST AND SET?**

Mesmo com a aplica√ß√£o dessa estrat√©gia, o Test And Set (TAS) continua proporcionando um grande overhead, pois essa opera√ß√£o at√¥mica indica o dado como sujo sempre que √© chamada, fazendo com que haja uma grande demanda de leitura e escrita na mem√≥ria mais distante, proporcionando uma sobrecarga dos barramentos compartilhados. 

No TTAS h√° uma melhoria desse resultado, dado que se evita ao m√°ximo realizar um TAS direto, de maneira que os dados ser√£o lidos do cache at√© que um deles esteja marcado como inv√°lido. 

Em um sistema operando em multithreds, a thread que primeiro entra na regi√£o cr√≠tica l√™ o dado de maneira atualizada, e as demais passam a ler desse cache, descongestionando o barramento compartilhado. Quando essa thread sai da regi√£o cr√≠tica, ela invalida o dado e acorda as demais threads, que tentam realizar um TAS, mas apenas uma consegue (por ser uma opera√ß√£o at√¥mica), fazendo com que ela tome a regi√£o cr√≠tica e o processo se repita. 

# Implementando Spinlocks

At√© o momento, o problema que estamos tentando resolver com as solu√ß√µes propostas √© a **conten√ß√£o**. 

![N4526UMP2TEO3PN9.gif](assets_resumo_2/N4526UMP2TEO3PN9.gif)

<aside>
üîÑ **RELEMBRANDO**

A conten√ß√£o ocorre quando temos um grande hotspot na concorr√™ncia e todas as threads tentam acessar esse ponto ao mesmo tempo, ocasionando uma perda do desempenho dada esta disputa.

</aside>

Al√©m disso, como visto nos casos acima estudados, algumas vezes limita√ß√µes, contexto e outras situa√ß√µes s√£o t√£o importantes quanto simplesmente o funcionamento e a implementa√ß√£o de uma trava.

## Trava de Anderson

A **trava de Anderson** pode ser compreendida como uma fila de threads. Esse algoritmo para implementa√ß√£o de spinlock utiliza a estrat√©gia FIFO para realizar espera ocupada em um sistema multithreads quando h√° a tentativa de ganhar a regi√£o cr√≠tica. 

A implementa√ß√£o √© feita com base em um array e utiliza de opera√ß√µes at√¥micas para incrementar o √≠ndice de acesso da posi√ß√£o que uma thread est√° fazendo spin.

Nessa abordagem, cada thread faz spin em sua pr√≥pria vari√°vel enquanto essa assume *false* como valor. Quando a posi√ß√£o da vari√°vel da thread est√° como *true*, isso significa dizer que esta thread est√° de posse da trava da regi√£o, isto √©, quando a thread atual deixar a regi√£o cr√≠tica, ela poder√° entrar, dado que √© sua vez, enquanto as demais est√£o fazendo spin. Quando uma thread est√° na regi√£o cr√≠tica, ela passa a posse da trava para a vari√°vel seguinte (seguindo o esquema de FIFO).

![2O6PF5M2PX7WCIBP.gif](assets_resumo_2/2O6PF5M2PX7WCIBP.gif)

**PONTOS POSITIVOS DA TRAVA DE ANDERSON**

- **Uso de uma opera√ß√£o at√¥mica para incremento do √≠ndice que ir√° ganhar a trava**
    
    Como j√° √© sabido, **opera√ß√µes at√¥micas n√£o s√£o bloqueantes**, portanto, ao utiliz√°-la na implementa√ß√£o dessa trava para realizar o incremento da flag, evitamos de gerar uma regi√£o cr√≠tica dentro da pr√≥pria trava. Al√©m disso, opera√ß√µes at√¥micas s√£o pouco custosas para nosso contexto.
    
- **O processo para entrada na regi√£o cr√≠tica √© sequenciado por meio de uma fila**
    
    Sequenciar a entrada na regi√£o cr√≠tica evita a ocorr√™ncia de um grande overhead, de deadlock e de starvation das threads que concorrem por ela, dado que, garantindo que a thread deixe a regi√£o (fa√ßa unlock), todas as threads ir√£o poder ganhar a regi√£o cr√≠tica no seu momento. 
    
- **Cada thread faz spin em uma vari√°vel privada**
    
    N√£o h√° concorr√™ncia das vari√°veis que atuam como flag, dado que toda thread atua na sua pr√≥pria vari√°vel privada. 
    
- **Implementado em uma fila circular**
    
    O acesso nessa estrutura de dados √© realizado em O(n) e h√° uma representa√ß√£o de infinitude, dado que √© circular. 
    

**PONTOS FALTANTES DA TRAVA DE ANDERSON**

- **N√∫mero de posi√ß√µes tem que ser proporcional ao n√∫mero de threads**
    
    Para que o algoritmo tenha um bom funcionamento, o n√∫mero de posi√ß√µes da fila tem que ser proporcional ao n√∫mero de threads operantes no sistema, tendo que este valor deve ser previamente conhecido, mas se trata de uma informa√ß√£o geralmente conhecida dada a arquitetura do sistema computacional. 
    

[‚Üí V√çDEO RESUMIDO SOBRE TRAVA DE ANDERSON](https://www.notion.so/assets_resumo_2-575677e675fe4395bcf951990f26360e)

## Trava CLH

A trava CLH tamb√©m opera em um esquema de fila de prioridade, por√©m com o uso de uma lista encadeada, ao inv√©s de um array, como na trava de Anderson.

Nessa abordagem, a calda da lista aponta inicialmente sempre para um n√≥ falso, indicando que a regi√£o cr√≠tica est√° liberada. Com a chegada de uma thread interessada em ocupar a regi√£o cr√≠tica, esta cria um n√≥ verdadeiro, que ser√° agora apontado pela calda da fila, e tamb√©m cria um apontador para n√≥ falso inicialmente apontado pela calda. 

No momento do unlock() essa thread muda a sua flag para falso, de modo que, se existe outra flag na fila, essa ir√° observar que a regi√£o cr√≠tica foi liberada, e faz um swap dela com o predecessor (a thread sai da fila).

A l√≥gica por tr√°s disso √© que cada thread √© capaz de observar qual o estado da thread predecessora a ela, com um apontador para sua flag. Se esta flag est√° como verdadeira, existem dois cen√°rios poss√≠veis: a thread est√° na regi√£o cr√≠tica, ou a thread tem prioridade em ganhar a regi√£o cr√≠tica. Se a flag estiver como falsa, isso indica que a posse da trava √© da thread atual. 

A lista encadeada criada pela CLH √© indireta, isto √©, ela √© compreendida pelo fato de cada thread apontar para dois endere√ßos de flag, mas isso n√£o √© feito de maneira expl√≠cita (como em uma implementa√ß√£o de linkedlist convencional, onde temos n√≥s com apontadores diretos uns para os outros).

Al√©m disso, √© v√°lido salientar que uma thread faz spin sempre em seu cache local, o que faz com que essa solu√ß√£o seja mais eficiente do ponto de vista de acesso √† mem√≥ria. Por√©m, esse mesmo fator faz com que, em algumas arquiteturas que n√£o possuem sistema de cache local, como a NANO, essa estrat√©gia passe a ser bem mais custosa, dado que o acesso ter√° que ser realizado em uma mem√≥ria local de um outro processador.

![S9KAH0KTTT5J4UQ7.gif](assets_resumo_2/S9KAH0KTTT5J4UQ7.gif)

## Trava MCS

A implementa√ß√£o de spinlock por MCS √© uma melhoria das anteriores, resolvendo o problema visto na CLH. 

De mesmo modo, utiliza de uma lista encadeada para implementar uma fila de prioridade, por√©m, desta vez, de maneira expl√≠cita. 

A calda da lista inicia apontando para um n√≥ NIL, indicando que n√£o h√° thread ocupando a regi√£o cr√≠tica. Quando uma thread chega interessada em ocupar a regi√£o cr√≠tica, a calda aponta para este n√≥, que apontar√° para o n√≥ NIL. Desse modo, essa thread poder√° ganhar a regi√£o. 

Caso uma thread chegue em seguida, a calda tamb√©m apontar√° para o n√≥ de interesse dela, que apontar√° para o n√≥ de interesse da thread que chegou anteriormente. Quando a thread da regi√£o cr√≠tica faz unlock, esta envia um sinal para a thread que est√° esperando por ela, fazendo com que o apontador mude para NIL, entregando a posse da trava para ela. 

![YWU4FZUGQVFH98CZ.gif](assets_resumo_2/YWU4FZUGQVFH98CZ.gif)

# Objetos concorrentes

Quando estamos interessados em implementar estruturas de dados e objetos que necessitam de concorr√™ncia, temos que levar em considera√ß√£o alguns cuidados. 

Assim como estamos interessados em manter algumas propriedades anteriormente faladas, como corretude e progresso, al√©m de evitar conten√ß√£o, estamos ainda preocupados com a garantia de **corretude**.

Por√©m, dependendo do contexto, o conceito de corretude pode ultrapassar o funcionamento esperado da estrutura de dados. Por exemplo, algumas vezes estamos mais preocupados com o desempenho do que com a atualiza√ß√£o dos dados, e por isso podemos abrir m√£o da segunda, tomando como base um SGBD. Para alguns sistemas, o dado atualizado √© bastante importante (como em sistemas banc√°rios, por exemplo, o que faz com que esses tenham que abrir m√£o de alto desempenho), mas em outros isso √© pass√°vel, sendo aceit√°vel a recupera√ß√£o de dados mais antigos.

Para isso, podemos flexibilizar o conceito de corretude, fazendo com que os objetos possam ter diferentes tipos de **consist√™ncia**. 

## Modelos de consist√™ncia

Modelos de consist√™ncia s√£o necess√°rios, pois muitas vezes necessitamos de haver replica√ß√£o para o ganho de desempenho. Desse modo, queremos ter algumas garantias de como as partes de um sistema concorrente ir√£o observar as opera√ß√µes para **garantir a corretude de funcionamento do contexto**. 

Como j√° falado anteriormente, para alguns dados contextos podemos abrir m√£o, ou melhor, flexibilizar a ideia de corretude para ganharmos em desempenho, j√° em outros, √© necess√°rio deixar o desempenho como uma menor prioridade para garantir a corretude do que o objeto se prop√µe. 

### Consist√™ncia estrita, at√¥mica ou linearizabilidade

A consist√™ncia estrita est√° vinculada ao **tempo** global do sistema, de modo que todos os eventos acontecem para todas as partes na ordem e no tempo que est√£o determinados na sequ√™ncia.

O n√≠vel de restri√ß√£o dessa solu√ß√£o √© bastante alto, pois todos os eventos se colocam praticamente de maneira sequencial. 

### Consist√™ncia sequencial

Na consist√™ncia sequencial podemos dizer que o efeito de cada execu√ß√£o nas partes concorrentes √© o mesmo que seria obtido para todas as leituras e escritas se essas fossem executadas em uma ordem sequencial, sem invers√µes dentro de cada programa. 

<aside>
üí° √â poss√≠vel **descrever uma sequencia de eventos** que √© v√°lida para a execu√ß√£o de todas as threads, isto √©, todas as threads envolvidas **percebem a mesma ordem dos eventos** para uma dada execu√ß√£o.

</aside>

**EXEMPLO 1 - TEM CONSIST√äNCIA SEQUENCIAL**

![Captura de tela de 2022-07-25 08-19-00.png](assets_resumo_2/Captura_de_tela_de_2022-07-25_08-19-00.png)

q.enq(x), q.enq(y), q.deq(x), q.deq(y) ‚úÖ

**EXEMPLO 2 - TEM CONSIST√äNCIA SEQUENCIAL**

![Captura de tela de 2022-07-25 08-24-48.png](assets_resumo_2/Captura_de_tela_de_2022-07-25_08-24-48.png)

p2.write(b), p3.read(b), p4.read(b), p1.write(a), p3.read(a), p4.read(a) ‚úÖ

**EXEMPLO 3 - N√ÉO TEM CONSISTENCIA SEQUENCIAL**

![Captura de tela de 2022-07-25 08-37-26.png](assets_resumo_2/Captura_de_tela_de_2022-07-25_08-37-26.png)

p1.write(a), p4.read(a) ‚Ä¶ ‚ùå

p2.write(b), p3.read(b) ‚Ä¶ ‚ùå

A consist√™ncia sequencial √© mais flex√≠vel do que a consist√™ncia estrita, mas ainda mant√©m um alto n√≠vel de inflexibilidade. Isso se d√° ao fato de que, mesmo que ela n√£o necessite de restri√ß√£o temporal, h√° a exigencia de que **todas as threads sejam necessariamente executadas em concordancia com a mesma sequencia de eventos**. 

### Consist√™ncia causal

Na consist√™ncia causal, apenas os eventos que apresentam **correla√ß√£o de causa e efeito** devem ser vistos na **mesma ordem por todas as threads**.

Podemos interpretar como uma rela√ß√£o de causa e efeito, por exemplo, a leitura e a escrita de um valor na mem√≥ria: para que eu possa ler o valor A, necessariamente tenho que ter escrito por √∫ltimo o valor A na mem√≥ria. 

**EXEMPLO 1**

![Captura de tela de 2022-07-25 08-44-59.png](assets_resumo_2/Captura_de_tela_de_2022-07-25_08-44-59.png)

**p1.write(a), p2.read(a), p3.read(a), p4.read(a)** ‚úÖ

Perceba que, necessariamente, o valor **a** tinha que estar escrito na mem√≥ria para que os processos **P2**, **P3** e **P4** possam ser executados com consist√™ncia. J√° para as demais escritas, isso pouco importa: a escrita de P1 do valor c e a escrita de P2 do valor b s√£o concorrentes e podem acontecer e serem percebidas de maneiras diferentes pelos processos P3 e P4.

**p1.write(a), p2.read(a), p3.read(a), p4.read(a)**, ****p1.write(c), p3.write(c), p2.write(b), p3.read(b) ‚úÖ //P3 percebeu essa ordem

**p1.write(a), p2.read(a), p3.read(a), p4.read(a)**, p2.write(b), p4.read(b), p1.write(c), p4.read(c) ‚úÖ //P4 percebeu essa ordem

**EXEMPLO 2**

![Captura de tela de 2022-07-25 08-55-48.png](assets_resumo_2/Captura_de_tela_de_2022-07-25_08-55-48.png)

p1.write(a), p2.read(a), p3.read(a) ‚Ä¶ ‚ùå

Para garantir a consist√™ncia causal √© necess√°rio que todas as rela√ß√µes de causalidade sejam garantidas. No exemplo acima temos uma contradi√ß√£o nesse sentido:

Em P3, para que possamos R(x)a, √© necess√°rio que tenhamos R(x)b. Para tal, √© necess√°rio que, em P2, W(x)b tenha sido realizado, mas para isso R(x)a tem que ter sido feito anteriormente. Para que R(X)a tenha sido feito em P2, W(x)a tem que ter sido realizado em P1. Por√©m, perceba que nessa ordem de causalidade √© necess√°rio que x tenha sido em algum momento sobrescrito, fazendo com que seja imposs√≠vel garantir que P3 aconte√ßa, dada a causalidade. 

**EXEMPLO 3**

![Captura de tela de 2022-07-25 09-03-43.png](assets_resumo_2/Captura_de_tela_de_2022-07-25_09-03-43.png)

p1.write(a), p4.read(a), p2.write(b), p4.read(b) ‚úÖ

p2.write(b), p3.read(b), p1.write(a), p3.read(a) ‚úÖ

Perceba que as escritas de P1 e P2 s√£o totalmente concorrentes e podem ser percebidas de maneiras distintas pelos processos P3 e P4.

**EXEMPLO 4**

![Captura de tela de 2022-07-25 09-06-52.png](assets_resumo_2/Captura_de_tela_de_2022-07-25_09-06-52.png)

p1.write_x(a), p2.read_x(a), p2.write_y(b) ‚úÖ

E se‚Ä¶

![Captura de tela de 2022-07-25 09-08-25.png](assets_resumo_2/Captura_de_tela_de_2022-07-25_09-08-25.png)

Qual pode ser o valor lido em R(x)‚ùì ?

p1.write_x(a), p2.read_x(a), p3.read_y(b), p3.read_x(a) ‚úÖ

‚ùì = a

Trivial! Mas, e se‚Ä¶

![Captura de tela de 2022-07-25 09-11-38.png](assets_resumo_2/Captura_de_tela_de_2022-07-25_09-11-38.png)

p1.write_x(a), p2.read_x(a), p2.write_y(b), p3.read_y(b), p4.read_x(a), p4.read_y(b), p3.read_x(a) ‚úÖ

R(x)‚ùì = a

R(y)‚ùì = b

Essa sequencia √© v√°lida e pode acontecer, mas existe um outro cen√°rio! P2 e P4 n√£o mant√©m rela√ß√£o causal, isto √©, a leitura de R(y)? em P4 pode ter acontecido antes da escrita de W(y)b, fazendo com que o resultado da leitura possa ser:

R(x)‚ùì = a

R(y)‚ùì = NIL

### Consist√™ncia quiescente ou eventual

Na consist√™ncia eventual, h√° a garantia apenas de que, em algum momento, as opera√ß√µes ser√£o realizadas e as altera√ß√µes ser√£o vis√≠veis para todas as threads. 

Um exemplo de consist√™ncia quiescente na pr√°tica s√£o os sistemas web e tamb√©m o DNS: os dados alterados s√£o propagados de maneira n√£o imediata, mas de maneira que em algum momento todos os interessados ir√£o perceber as modifica√ß√µes realizadas.

## Como escolher o modelo de consist√™ncia?

| MODELO | CARACTER√çTICAS | APLICABILIDADE |
| --- | --- | --- |
| CONSIST√äNCIA AT√îMICA | √â de f√°cil compreens√£o; a ordem temporal em diferentes threads √© de extrema import√¢ncia; n√£o exige bloqueio, dado que uma opera√ß√£o acontece sempre ap√≥s outra.  | Objetos que ser√£o utilizados como blocos ou que necessitam de ordem temporal de opera√ß√£o e atualiza√ß√£o. |
| CONSIST√äNCIA SEQUENCIAL | N√£o leva a ordem temporal em considera√ß√£o, o que implica que a ordem de execu√ß√£o pode ser alterada entre threads; n√£o exige bloqueio, dado que considera-se uma sequ√™ncia de opera√ß√µes. | Objetos s√£o independentes, isto √©, quando as leituras e escritas podem ser reordenadas entre as threads. |
| CONSIST√äNCIA CAUSAL | Necessita de amarra apenas em opera√ß√µes que envolvem causa e efeito; n√£o exige bloqueio, dado que, mantendo as rela√ß√µes de causa e efeito, as demais podem ser interpretados de maneiras diferentes pelas demais threads. | Otimiza√ß√µes de sistemas distribu√≠dos. |
| CONSIST√äNCIA QUIESCENTE | Dado o contexto, existe uma garantia de que em algum momento, todas as threads ir√£o observar as opera√ß√µes realizadas. | Sistemas web, de compartilhamento de arquivos e propaga√ß√£o de DNS. |

## Corretude e progresso na pr√°tica

Derivado de assuntos anteriormente discutidos, podemos retomar que aumentar o **n√∫mero de processadores** **n√£o** vai necessariamente **aumentar a efici√™ncia do sistema** de maneira direta, dado que **sempre h√° uma parte sequencial escondida** que poder√° ocasionar um gargalo futuro no objeto (derivado da **Lei de Amdahl**).

### Sincroniza√ß√£o grossa

A sincroniza√ß√£o grossa diz respeito ao que j√° estamos acostumados a perceber em exemplos j√° vistos de sincroniza√ß√£o:

```java
lock();
criticalAreaCode();
unlock();
```

Ou seja, h√° o **travamento** da regi√£o cr√≠tica (ou a tentativa de faz√™-lo) antes de adentrar a mesma e o **destravamento** na sa√≠da. 

Sabemos que esse princ√≠pio **garante corretude**, pois a exclus√£o m√∫tua aplicada nessas t√©cnicas impede que outras threads possam executar o bloco de c√≥digo ao mesmo tempo. 

Por√©m, a sincroniza√ß√£o grossa pode ocasionar **gargalos**, pois as threads que tentam realizar o lock() da regi√£o ficar√£o em **espera** (ou perdem o processador, no caso de um block) at√© que a atual fa√ßa unlock(), reduzindo em parte a efici√™ncia do sistema. 

**EXEMPLO DO HASHMAP**

Podemos imaginar a ED de um HashMap. Sabemos que ele seque um esquema de tabela hash, na qual uma chave aponta para uma cole√ß√£o de elementos, como no exemplo abaixo:

| KEY | 0 | 1 | 2 | ‚Ä¶ | n |
| --- | --- | --- | --- | --- | --- |
| KEY 1 |  |  |  | ‚Ä¶ |  |
| KEY 2 |  |  |  | ‚Ä¶ |  |
| KEY 3 |  |  |  | ‚Ä¶ |  |

Utilizando sincroniza√ß√£o grossa, para adicionar um elemento a esse HashMap, ele por inteiro seria travado para que uma chave fosse acessada e um elemento fosse escrito na cole√ß√£o apontada por essa chave. Essa decis√£o ir√° provocar a espera de outras threads que est√£o acessando essa ED n√£o necessariamente no mesmo apontador.

**lock()**

| KEY | 0 | 1 | 2 | ‚Ä¶ | n |
| --- | --- | --- | --- | --- | --- |
| KEY 1 |  |  |  | ‚Ä¶ |  |
| KEY 2 |  |  |  | ‚Ä¶ |  |
| KEY 3 |  |  |  | ‚Ä¶ |  |

**unlock()**

### Sincroniza√ß√£o granular

Na sincroniza√ß√£o granular, ao inv√©s de realizar um lock() na estrutura completa, este √© realizado em partes da estrutura. Desse modo, pensando em objetos concorrentes, a parte da estrutura de dados que ser√° manipulada ser√° travada, enquanto as demais ficar√£o livres para acesso.

**RETOMANDO O EXEMPLO DO HASHMAP**

Ainda observando o exemplo do HashMap: se ao inv√©s de realizarmos um lock() em toda a ED, resolvermos faz√™-lo em uma por√ß√£o menor, neste caso, para cada chave que √© acessada para uma opera√ß√£o de escrita, apenas ela √© travada, deixando as demais livres para serem acessadas.

**lock(KEY 2)**

| KEY | 0 | 1 | 2 | ‚Ä¶ | n |
| --- | --- | --- | --- | --- | --- |
| KEY 1 |  |  |  | ‚Ä¶ |  |
| KEY 2 |  |  |  | ‚Ä¶ |  |
| KEY 3 |  |  |  | ‚Ä¶ |  |

**unlock(KEY 2)**

### Sincroniza√ß√£o otimista

Na sincroniza√ß√£o otimista, as opera√ß√µes de altera√ß√£o em uma regi√£o da estrutura de dados √© realizada sem haver o travamento da regi√£o durante a execu√ß√£o da opera√ß√£o. Isso pode ser realizado por meio de uma verifica√ß√£o ou com a altera√ß√£o de c√≥pias. 

Um ponto positivo dessa abordagem de sincroniza√ß√£o √© que enquanto uma opera√ß√£o de altera√ß√£o √© realizada por uma thread em uma regi√£o, esta n√£o est√° bloqueada para as demais. Por√©m, um ponto negativo, √© que a falta de travamento da regi√£o pode fazer com quem uma thread mais r√°pida possa realizar uma altera√ß√£o antes que a thread corrente conclua a sua, tendo que o esfor√ßo ser realizado novamente. 

**MAIS UMA VEZ O HASHMAP**

No exemplo do HashMap podemos tamb√©m aplicar a sincroniza√ß√£o otimista. Imagine que ao inv√©s de fazer o lock() da estrutura de dados como um todo, ou realizar o lock() apenas de um apontador, agora optamos por realizar uma c√≥pia da regi√£o do apontador, alter√°-la e depois substitu√≠-la.

| KEY | 0 | 1 | 2 | ‚Ä¶ | n |
| --- | --- | --- | --- | --- | --- |
| KEY 1 |  |  |  | ‚Ä¶ |  |
| KEY 2 |  |  |  | ‚Ä¶ |  |
| KEY 3 |  |  |  | ‚Ä¶ |  |

*C√≥pia de KEY 2 realizada para ser alterada pela thread*

| KEY 2 |  |  |  | ‚Ä¶ |  |
| --- | --- | --- | --- | --- | --- |

*Com isso, temos dois cen√°rios poss√≠veis:*

**CEN√ÅRIO 1**: *nada foi alterado enquanto as altera√ß√µes eram realizadas na c√≥pia. Assim, podemos apenas alterar o valor do apontador para a c√≥pia que foi alterada.*

| KEY | 0 | 1 | 2 | ‚Ä¶ | n |
| --- | --- | --- | --- | --- | --- |
| KEY 1 |  |  |  | ‚Ä¶ |  |
| KEY 2 | x |  |  | ‚Ä¶ |  |
| KEY 3 |  |  |  | ‚Ä¶ |  |

**CEN√ÅRIO 2**: *outra thread alterou a regi√£o enquanto est√°vamos manipulando a c√≥pia. Tendo isto como acontecimento, precisamos refazer os passos com o novo estado da estrutura de dados.*

| KEY | 0 | 1 | 2 | ‚Ä¶ | n |
| --- | --- | --- | --- | --- | --- |
| KEY 1 |  |  |  | ‚Ä¶ |  |
| KEY 2 | a |  |  | ‚Ä¶ |  |
| KEY 3 |  |  |  | ‚Ä¶ |  |

| KEY 2 | x |  |  | ‚Ä¶ |  |
| --- | --- | --- | --- | --- | --- |

*A c√≥pia √© insconsistente, portanto precisa ser refeita e realterada.*

| KEY 2 | a | x |  | ‚Ä¶ |  |
| --- | --- | --- | --- | --- | --- |

| KEY | 0 | 1 | 2 | ‚Ä¶ | n |
| --- | --- | --- | --- | --- | --- |
| KEY 1 |  |  |  | ‚Ä¶ |  |
| KEY 2 | a | x |  | ‚Ä¶ |  |
| KEY 3 |  |  |  | ‚Ä¶ |  |

### Sincroniza√ß√£o pregui√ßosa

A sincroniza√ß√£o pregui√ßosa leva em considera√ß√£o que uma altera√ß√£o realizada por uma regi√£o cr√≠tica pode ser ‚Äúmarcada como realizada‚Äù para ser feita posteriormente. Essa solu√ß√£o tamb√©m n√£o necessita de uma trava imediata, as threads que observam a estrutura de dados podem apenas observar se as flags de altera√ß√£o foram levantadas para uma entidade.

**HASHMAP, O RETORNO**

Ainda no exemplo do HashMap: imagine que para implementar o m√©todo de remo√ß√£o de um elemento apenas pud√©ssemos marcar o elemento como removido e realizar todas as remo√ß√µes em um momento de pouco acesso da estrutura (um flush de remo√ß√µes).

Desse modo, se na visualiza√ß√£o abaixo for de nosso interesse remover o x de KEY 2, poder√≠amos apenas faz√™-lo da seguinte forma:

| KEY | 0 | 1 | 2 | ‚Ä¶ | n |
| --- | --- | --- | --- | --- | --- |
| KEY 1 |  |  |  | ‚Ä¶ |  |
| KEY 2 | a | x |  | ‚Ä¶ |  |
| KEY 3 |  |  |  | ‚Ä¶ |  |

Assim, se outra thread necessita acessar esse dado, ela poder√° verificar que o dado foi marcado como removido, considerando uma remo√ß√£o l√≥gica. 

### Sincroniza√ß√£o livre de travas

Na sincroniza√ß√£o livre de travas toda sincroniza√ß√£o √© realizada por meio de opera√ß√µes at√¥micas, sem a necessidade de travamento de regi√µes, apenas realizando opera√ß√µes indivis√≠veis. 

Nessa solu√ß√£o, uma thread n√£o pode impedir outra de fazer progresso, dado que nenhuma delas est√° realizando o travamento de uma regi√£o cr√≠tica, por√©m o overhead da solu√ß√£o pode ser alto, pelo mesmo motivo, al√©m da complexidade de implementa√ß√£o.